{
    "$schema": "../schemas/mlx-models.schema.json",
    "modelsPath": "~/.mlx/models",
    "batchSize": 1,
    "temperature": 0.7,
    "embedding_models": {
        "qwen3-0.6b": {
            "name": "Qwen3-Embedding-0.6B",
            "path": "/Volumes/ExternalSSD/ai/models/huggingface/hub/models--Qwen--Qwen3-Embedding-0.6B",
            "dimensions": 768,
            "max_tokens": 8192,
            "recommended_for": [
                "quick_search",
                "development"
            ],
            "transformers_model": "Qwen/Qwen3-Embedding-0.6B",
            "memory_gb": 1.0
        },
        "qwen3-4b": {
            "name": "Qwen3-Embedding-4B",
            "path": "/Volumes/ExternalSSD/ai/models/huggingface/hub/models--Qwen--Qwen3-Embedding-4B",
            "dimensions": 768,
            "max_tokens": 8192,
            "recommended_for": [
                "production",
                "balanced_performance"
            ],
            "transformers_model": "Qwen/Qwen3-Embedding-4B",
            "memory_gb": 4.0
        },
        "qwen3-8b": {
            "name": "Qwen3-Embedding-8B",
            "path": "/Volumes/ExternalSSD/ai/models/huggingface/hub/models--Qwen--Qwen3-Embedding-8B",
            "dimensions": 768,
            "max_tokens": 8192,
            "recommended_for": [
                "high_accuracy",
                "research"
            ],
            "transformers_model": "Qwen/Qwen3-Embedding-8B",
            "memory_gb": 8.0
        }
    },
    "reranker_models": {
        "qwen3-reranker": {
            "name": "Qwen3-Reranker-4B",
            "path": "/Volumes/ExternalSSD/ai/models/huggingface/hub/models--Qwen--Qwen3-Reranker-4B",
            "transformers_model": "Qwen/Qwen3-Reranker-4B",
            "max_pairs": 1000,
            "memory_gb": 4.0,
            "recommended_for": [
                "production_reranking",
                "search_optimization"
            ]
        }
    },
    "chat_models": {
        "glm-4.6": {
            "name": "GLM-4.6-4bit",
            "path": "${HF_HOME}/hub/models--mlx-community--GLM-4.6-4bit",
            "transformers_model": "mlx-community/GLM-4.6-4bit",
            "quantization": "4bit",
            "context_length": 32768,
            "memory_gb": 8.0,
            "priority": 1,
            "recommended_for": [
                "coding",
                "refactoring",
                "debugging",
                "documentation",
                "general_purpose",
                "enhanced_reasoning"
            ],
            "coding_tasks": [
                "general",
                "refactoring",
                "debugging",
                "documentation",
                "code_generation"
            ]
        },
        "kimi-k2-instruct": {
            "name": "Kimi-K2-Instruct-4bit",
            "path": "${HF_HOME}/hub/models--mlx-community--Kimi-K2-Instruct-4bit",
            "transformers_model": "mlx-community/Kimi-K2-Instruct-4bit",
            "quantization": "4bit",
            "context_length": 32768,
            "memory_gb": 6.0,
            "priority": 2,
            "recommended_for": [
                "general_purpose",
                "instruction_following",
                "reasoning",
                "multilingual"
            ],
            "coding_tasks": [
                "general",
                "code_review",
                "explanation",
                "multilingual_code"
            ]
        },
        "kimi-vl-thinking": {
            "name": "Kimi-VL-Thinking-4bit",
            "path": "/Volumes/ExternalSSD/ai-models/huggingface-models/models--mlx-community--Kimi-VL-Thinking-4bit",
            "transformers_model": "mlx-community/Kimi-VL-Thinking-4bit",
            "quantization": "4bit",
            "context_length": 16384,
            "memory_gb": 7.0,
            "priority": 3,
            "recommended_for": [
                "visual_tasks",
                "multimodal",
                "thinking_tasks",
                "reasoning"
            ],
            "coding_tasks": [
                "general",
                "visual_debugging",
                "documentation",
                "ui_tasks"
            ]
        },
        "kimi-dev-72b": {
            "name": "Kimi-Dev-72B-8bit",
            "path": "/Volumes/ExternalSSD/ai-models/huggingface-models/models--mlx-community--Kimi-Dev-72B-8bit",
            "transformers_model": "mlx-community/Kimi-Dev-72B-8bit",
            "quantization": "8bit",
            "context_length": 65536,
            "memory_gb": 48.0,
            "priority": 4,
            "recommended_for": [
                "complex_reasoning",
                "development_tasks",
                "advanced_coding",
                "system_design"
            ],
            "coding_tasks": [
                "architecture",
                "advanced_algorithms",
                "system_design",
                "code_generation",
                "optimization"
            ]
        },
        "qwen2.5-0.5b": {
            "name": "mlx-community/Qwen2.5-0.5B-Instruct-4bit",
            "path": "${MLX_CACHE_DIR}/hub/models--mlx-community--Qwen2.5-0.5B-Instruct-4bit",
            "transformers_model": "mlx-community/Qwen2.5-0.5B-Instruct-4bit",
            "quantization": "4bit",
            "context_length": 32768,
            "memory_gb": 1.0,
            "recommended_for": [
                "fast_inference",
                "development",
                "testing"
            ],
            "coding_tasks": [
                "quick_fixes",
                "simple_generation"
            ]
        },
        "qwen2.5-vl": {
            "name": "mlx-community/Qwen2.5-VL-3B-Instruct-6bit",
            "path": "${MLX_CACHE_DIR}/hub/models--mlx-community--Qwen2.5-VL-3B-Instruct-6bit",
            "transformers_model": "mlx-community/Qwen2.5-VL-3B-Instruct-6bit",
            "quantization": "6bit",
            "supports_vision": true,
            "context_length": 32768,
            "memory_gb": 6.0,
            "recommended_for": [
                "multimodal",
                "vision_tasks",
                "image_analysis"
            ],
            "coding_tasks": [
                "ui_analysis",
                "diagram_interpretation",
                "visual_debugging"
            ]
        },
        "qwen3-coder-7b": {
            "name": "qwen3-coder-7b-mlx",
            "path": "/Volumes/ExternalSSD/ai-models/local-models/qwen3-coder-7b-mlx",
            "transformers_model": "mlx-community/qwen3-coder-7b-mlx",
            "quantization": "4bit",
            "context_length": 16384,
            "memory_gb": 8.0,
            "priority": 2,
            "recommended_for": [
                "code_generation",
                "completion",
                "simple_fixes"
            ],
            "coding_tasks": [
                "code_generation",
                "completion",
                "simple_fixes"
            ]
        },
        "qwen3-coder-30b": {
            "name": "mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit",
            "path": "${MLX_CACHE_DIR}/hub/models--mlx-community--Qwen3-Coder-30B-A3B-Instruct-4bit",
            "transformers_model": "mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit",
            "quantization": "4bit",
            "context_length": 32768,
            "memory_gb": 32.0,
            "priority": 3,
            "recommended_for": [
                "architecture",
                "complex_refactoring",
                "large_context",
                "system_design"
            ],
            "coding_tasks": [
                "architecture",
                "complex_refactoring",
                "large_codebase",
                "system_design"
            ]
        },
        "mixtral-8x7b": {
            "name": "mlx-community/Mixtral-8x7B-v0.1-hf-4bit-mlx",
            "path": "${MLX_CACHE_DIR}/hub/models--mlx-community--Mixtral-8x7B-v0.1-hf-4bit-mlx",
            "transformers_model": "mlx-community/Mixtral-8x7B-v0.1-hf-4bit-mlx",
            "quantization": "4bit",
            "type": "mixture_of_experts",
            "context_length": 32768,
            "memory_gb": 32.0,
            "recommended_for": [
                "complex_reasoning",
                "multilingual",
                "expert_domain_tasks"
            ],
            "coding_tasks": [
                "complex_reasoning",
                "multilingual_code",
                "domain_expertise"
            ]
        },
        "phi3-mini": {
            "name": "mlx-community/Phi-3-mini-4k-instruct-4bit",
            "path": "${MLX_CACHE_DIR}/hub/models--mlx-community--Phi-3-mini-4k-instruct-4bit",
            "transformers_model": "mlx-community/Phi-3-mini-4k-instruct-4bit",
            "quantization": "4bit",
            "context_length": 4096,
            "memory_gb": 4.0,
            "recommended_for": [
                "efficient_inference",
                "mobile",
                "quick_tasks"
            ],
            "coding_tasks": [
                "quick_fixes",
                "simple_generation",
                "code_review"
            ]
        },
        "gemma-2-2b": {
            "name": "mlx-community/gemma-2-2b-it-4bit",
            "path": "/Volumes/ExternalSSD/huggingface_cache/models--mlx-community--gemma-2-2b-it-4bit",
            "transformers_model": "mlx-community/gemma-2-2b-it-4bit",
            "quantization": "4bit",
            "context_length": 8192,
            "memory_gb": 4.0,
            "recommended_for": [
                "efficient_inference",
                "google_ecosystem",
                "balanced_performance"
            ],
            "coding_tasks": [
                "general",
                "code_review",
                "documentation"
            ]
        },
        "smol-lm-135m": {
            "name": "mlx-community/SmolLM-135M-Instruct-4bit",
            "path": "/Volumes/ExternalSSD/ai-cache/huggingface/hub/models--mlx-community--SmolLM-135M-Instruct-4bit",
            "transformers_model": "mlx-community/SmolLM-135M-Instruct-4bit",
            "quantization": "4bit",
            "context_length": 2048,
            "memory_gb": 1.0,
            "recommended_for": [
                "ultra_light",
                "testing",
                "edge_devices"
            ],
            "coding_tasks": [
                "simple_fixes",
                "basic_completion",
                "testing"
            ]
        },
        "gemma-3-270m": {
            "name": "google/gemma-3-270m-it",
            "path": "${MLX_CACHE_DIR}/models--google--gemma-3-270m-it",
            "transformers_model": "google/gemma-3-270m-it",
            "quantization": "4bit",
            "context_length": 8192,
            "memory_gb": 0.5,
            "tier": "always_on",
            "priority": 5,
            "recommended_for": [
                "ultra_fast",
                "quick_responses",
                "utility_tasks",
                "always_available"
            ],
            "coding_tasks": [
                "simple_fixes",
                "quick_completion",
                "utility_generation",
                "fast_responses"
            ]
        }
    },
    "safety_models": {
        "llama_guard": {
            "name": "llamas-community/LlamaGuard-7b",
            "path": "${MLX_CACHE_DIR}/models--llamas-community--LlamaGuard-7b",
            "transformers_model": "llamas-community/LlamaGuard-7b",
            "memory_gb": 7.0,
            "recommended_for": [
                "content_moderation",
                "safety_checks",
                "harmful_content_detection"
            ]
        }
    },
    "default_models": {
        "embedding": "qwen3-4b",
        "reranker": "qwen3-reranker",
        "chat": "glm-4.6",
        "coding": "glm-4.6",
        "vision": "kimi-vl-thinking",
        "vision_secondary": "qwen2.5-vl",
        "large_context": "kimi-dev-72b",
        "lightweight": "qwen2.5-0.5b",
        "ultra_fast": "gemma-3-270m",
        "always_on": "gemma-3-270m",
        "general_purpose": "kimi-k2-instruct",
        "safety": "llama_guard"
    },
    "task_routing": {
        "quick_fix": "qwen3-coder-7b",
        "code_generation": "glm-4.6",
        "refactoring": "glm-4.6",
        "debugging": "glm-4.6",
        "documentation": "glm-4.6",
        "architecture": "kimi-dev-72b",
        "complex_analysis": "kimi-dev-72b",
        "performance_optimization": "kimi-dev-72b",
        "system_design": "kimi-dev-72b",
        "vision_tasks": "kimi-vl-thinking",
        "ui_analysis": "kimi-vl-thinking",
        "diagram_interpretation": "kimi-vl-thinking",
        "multilingual": "kimi-k2-instruct",
        "general_purpose": "kimi-k2-instruct",
        "instruction_following": "kimi-k2-instruct",
        "lightweight_tasks": "phi3-mini",
        "utility_tasks": "gemma-3-270m",
        "fast_responses": "gemma-3-270m",
        "always_available": "gemma-3-270m",
        "default": "glm-4.6"
    },
    "models": [
        {
            "name": "mlx-community/GLM-4.6-4bit",
            "size": "8B",
            "ram": "8GB",
            "maxTokens": 32768
        },
        {
            "name": "mlx-community/Kimi-K2-Instruct-4bit",
            "size": "7B",
            "ram": "6GB",
            "maxTokens": 32768
        },
        {
            "name": "mlx-community/Kimi-VL-Thinking-4bit",
            "size": "7B",
            "ram": "7GB",
            "maxTokens": 16384
        },
        {
            "name": "mlx-community/Kimi-Dev-72B-8bit",
            "size": "72B",
            "ram": "48GB",
            "maxTokens": 65536
        },
        {
            "name": "mlx-community/Qwen2.5-0.5B-Instruct-4bit",
            "size": "0.5B",
            "ram": "1GB",
            "maxTokens": 32768
        },
        {
            "name": "mlx-community/Qwen2.5-VL-3B-Instruct-6bit",
            "size": "3B",
            "ram": "6GB",
            "maxTokens": 32768
        },
        {
            "name": "mlx-community/qwen3-coder-7b-mlx",
            "size": "7B",
            "ram": "8GB",
            "maxTokens": 16384
        },
        {
            "name": "mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit",
            "size": "30B",
            "ram": "32GB",
            "maxTokens": 32768
        },
        {
            "name": "mlx-community/Mixtral-8x7B-v0.1-hf-4bit-mlx",
            "size": "56B",
            "ram": "32GB",
            "maxTokens": 32768
        },
        {
            "name": "mlx-community/Phi-3-mini-4k-instruct-4bit",
            "size": "3.8B",
            "ram": "4GB",
            "maxTokens": 4096
        },
        {
            "name": "mlx-community/gemma-2-2b-it-4bit",
            "size": "2B",
            "ram": "4GB",
            "maxTokens": 8192
        },
        {
            "name": "mlx-community/SmolLM-135M-Instruct-4bit",
            "size": "0.135B",
            "ram": "1GB",
            "maxTokens": 2048
        },
        {
            "name": "google/gemma-3-270m-it",
            "size": "0.27B",
            "ram": "0.5GB",
            "maxTokens": 8192
        }
    ]
}