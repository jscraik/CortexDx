{"id":"intro-chunk-0","pageId":"intro","url":"https://modelcontextprotocol.io/docs/getting-started/intro","title":"What is the Model Context Protocol (MCP)? - Model Context Protocol","text":"What is the Model Context Protocol (MCP)? - Model Context Protocol Skip to main content Model Context Protocol home page Search... âŒ˜ K Blog GitHub Search... Navigation Get started What is the Model Context Protocol (MCP)? Documentation Specification Community About MCP Get started What is MCP? About MCP Architecture Servers Clients Versioning Develop with MCP Connect to local MCP servers Connect to remote MCP Servers Build an MCP server Build an MCP client SDKs Security Developer tools MCP Inspector On this page What can MCP enable? Why does MCP matter? Start Building Learn more Get started What is the Model Context Protocol (MCP)? Copy page Copy page MCP (Model Context Protocol) is an open-source standard for connecting AI applications to external systems. Using MCP, AI applications like Claude or ChatGPT can connect to data sources (e.g. local files, databases), tools (e.g. search engines, calculators) and workflows (e.g. specialized prompts)â€”enabling them to access key information and perform tasks. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems. â€‹ What can MCP enable? Agents can access your Google Calendar and Notion, acting as a more personalized AI assistant. Claude Code can generate an entire web app using a Figma design. Enterprise chatbots can connect to multiple databases across an organization, empowering users to analyze data using chat. AI models can create 3D designs on Blender and print them out using a 3D printer. â€‹ Why does MCP matter? Depending on where you sit in the ecosystem, MCP can have a range of benefits. Developers : MCP reduces development time and complexity when building, or integrating with, an AI application or agent. AI applications or agents : MCP provides access to an ecosystem of data sources, tools and apps which will enhance capabilities and improve the end-user experience. End-users : MCP results in more capable AI applications or agents which can access your data and take actions on your behalf when necessary. â€‹ Start Building Build servers Create MCP servers to expose your data and tools Build clients Develop applications that connect to MCP servers â€‹ Learn more Understand concepts Learn the core concepts and architecture of MCP Was this page helpful? Yes No Architecture âŒ˜ I github","headings":[]}
{"id":"spec-2025-06-18-chunk-0","pageId":"spec-2025-06-18","url":"https://spec.modelcontextprotocol.io/specification/2025-06-18/basic/lifecycle/","title":"Versioning - Model Context Protocol","text":"Versioning - Model Context Protocol Skip to main content Model Context Protocol home page Search... âŒ˜ K Blog GitHub Search... Navigation About MCP Versioning Documentation Specification Community About MCP Get started What is MCP? About MCP Architecture Servers Clients Versioning Develop with MCP Connect to local MCP servers Connect to remote MCP Servers Build an MCP server Build an MCP client SDKs Security Developer tools MCP Inspector On this page Revisions Negotiation About MCP Versioning Copy page Copy page The Model Context Protocol uses string-based version identifiers following the format YYYY-MM-DD , to indicate the last date backwards incompatible changes were made. The protocol version will not be incremented when the protocol is updated, as long as the changes maintain backwards compatibility. This allows for incremental improvements while preserving interoperability. â€‹ Revisions Revisions may be marked as: Draft : in-progress specifications, not yet ready for consumption. Current : the current protocol version, which is ready for use and may continue to receive backwards compatible changes. Final : past, complete specifications that will not be changed. The current protocol version is 2025-06-18 . â€‹ Negotiation Version negotiation happens during initialization . Clients and servers MAY support multiple protocol versions simultaneously, but they MUST agree on a single version to use for the session. The protocol provides appropriate error handling if version negotiation fails, allowing clients to gracefully terminate connections when they cannot find a version compatible with the server. Was this page helpful? Yes No Clients Connect to local MCP servers âŒ˜ I github","headings":[]}
{"id":"about-chunk-0","pageId":"about","url":"https://modelcontextprotocol.io/about","title":"Model Context Protocol - Model Context Protocol","text":"Model Context Protocol - Model Context Protocol Skip to main content Model Context Protocol home page Search... âŒ˜ K Blog GitHub Search... Navigation Model Context Protocol Documentation Specification Community About MCP Connect your AI applications to the world AI-enabled tools are powerful, but theyâ€™re often limited to the information you manually provide or require bespoke integrations. Whether itâ€™s reading files from your computer, searching through an internal or external knowledge base, or updating tasks in an project management tool, MCP provides a secure, standardized, simple way to give AI systems the context they need. How it works 1 Choose MCP servers Pick from pre-built servers for popular tools like GitHub, Google Drive, Slack and hundreds of others. Combine multiple servers for complete workflows, or easily build your own for custom integrations. 2 Connect your AI application Configure your AI application (like Claude, VS Code, or ChatGPT) to connect to your MCP servers. The application can now see available tools, resources and prompts from all connected servers. 3 Work with context Your AI-powered application can now access real data, execute actions, and provide more helpful responses based on your actual context. Join a growing ecosystem 10 Official SDKs 90+ Compatible Clients 1000+ Available Servers Get Started âŒ˜ I","headings":[]}
{"id":"concepts-capabilities-chunk-0","pageId":"concepts-capabilities","url":"https://modelcontextprotocol.io/docs/concepts/capabilities","title":"Page Not Found","text":"Page Not Found Skip to main content Model Context Protocol home page Search... âŒ˜ K Blog GitHub Search... Navigation Page Not Found Documentation Specification Community About MCP Get started What is MCP? About MCP Architecture Servers Clients Versioning Develop with MCP Connect to local MCP servers Connect to remote MCP Servers Build an MCP server Build an MCP client SDKs Security Developer tools MCP Inspector 404 Page Not Found We couldn&#x27;t find the page. Maybe you were looking for one of these pages below? Build an MCP server Resources âŒ˜ I","headings":[]}
{"id":"concepts-tools-chunk-0","pageId":"concepts-tools","url":"https://modelcontextprotocol.io/docs/concepts/tools","title":"Tools - Model Context Protocol","text":"Tools - Model Context Protocol Skip to main content Model Context Protocol home page Version 2025-06-18 (latest) Search... âŒ˜ K Blog GitHub Search... Navigation Server Features Tools Documentation Specification Community About MCP Specification Key Changes Architecture Base Protocol Overview Lifecycle Transports Authorization Security Best Practices Utilities Client Features Roots Sampling Elicitation Server Features Overview Prompts Resources Tools Utilities Schema Reference On this page User Interaction Model Capabilities Protocol Messages Listing Tools Calling Tools List Changed Notification Message Flow Data Types Tool Tool Result Text Content Image Content Audio Content Resource Links Embedded Resources Structured Content Output Schema Error Handling Security Considerations Server Features Tools Copy page Copy page Protocol Revision : 2025-06-18 The Model Context Protocol (MCP) allows servers to expose tools that can be invoked by language models. Tools enable models to interact with external systems, such as querying databases, calling APIs, or performing computations. Each tool is uniquely identified by a name and includes metadata describing its schema. â€‹ User Interaction Model Tools in MCP are designed to be model-controlled , meaning that the language model can discover and invoke tools automatically based on its contextual understanding and the userâ€™s prompts. However, implementations are free to expose tools through any interface pattern that suits their needsâ€”the protocol itself does not mandate any specific user interaction model. For trust & safety and security, there SHOULD always be a human in the loop with the ability to deny tool invocations. Applications SHOULD : Provide UI that makes clear which tools are being exposed to the AI model Insert clear visual indicators when tools are invoked Present confirmation prompts to the user for operations, to ensure a human is in the loop â€‹ Capabilities Servers that support tools MUST declare the tools capability: Copy { \"capabilities\" : { \"tools\" : { \"listChanged\" : true } } } listChanged indicates whether the server will emit notifications when the list of available tools changes. â€‹ Protocol Messages â€‹ Listing Tools To discover available tools, clients send a tools/list request. This operation supports pagination . Request: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 1 , \"method\" : \"tools/list\" , \"params\" : { \"cursor\" : \"optional-cursor-value\" } } Response: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 1 , \"result\" : { \"tools\" : [ { \"name\" : \"get_weather\" , \"title\" : \"Weather Information Provider\" , \"description\" : \"Get current weather information for a location\" , \"inputSchema\" : { \"type\" : \"object\" , \"properties\" : { \"location\" : { \"type\" : \"string\" , \"description\" : \"City name or zip code\" } }, \"required\" : [ \"location\" ] } } ], \"nextCursor\" : \"next-page-cursor\" } } â€‹ Calling Tools To invoke a tool, clients send a tools/call request: Request: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 2 , \"method\" : \"tools/call\" , \"params\" : { \"name\" : \"get_weather\" , \"arguments\" : { \"location\" : \"New York\" } } } Response: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 2 , \"result\" : { \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Current weather in New York: \\n Temperature: 72Â°F \\n Conditions: Partly cloudy\" } ], \"isError\" : false } } â€‹ List Changed Notification When the list of available tools changes, servers that declared the listChanged capability SHOULD send a notification: Copy { \"jsonrpc\" : \"2.0\" , \"method\" : \"notifications/tools/list_changed\" } â€‹ Message Flow â€‹ Data Types â€‹ Tool A tool definition includes: name : Unique identifier for the tool title : Optional human-readable name of the tool for display purposes. description : Human-readable description of functionality inputSchema : JSON Schema defining expected parameters outputSchema : Optional JSON Schema defining expected output structure annotations : optional properties describing tool behavior For trust & safety and security, clients MUST consider tool annotations to be untrusted unless they come from trusted servers. â€‹ Tool Result Tool results may contain structured or unstructured content. Unstructured content is returned in the content field of a result, and can contain multiple content items of different types: All content types (text, image, audio, resource links, and embedded resources) support optional annotations that provide metadata about audience, priority, and modification times. This is the same annotation format used by resources and prompts. â€‹ Text Content Copy { \"type\" : \"text\" , \"text\" : \"Tool result text\" } â€‹ Image Content Copy { \"type\" : \"image\" , \"data\" : \"base64-encoded-data\" , \"mimeType\" : \"image/png\" \"annotations\" : { \"audience\" : [ \"user\" ], \"priority\" : 0.9 } } This example demonstrates the use of an optional Annotation. â€‹ Audio Content Copy { \"type\" : \"audio\" , \"data\" : \"base64-encoded-audio-data\" , \"mimeType\" : \"audio/wav\" } â€‹ Resource Links A tool MAY return links to Resources , to provide additional context or data. In this case, the tool will return a URI that can be subscribed to or fetched by the client: Copy { \"type\" : \"resource_link\" , \"uri\" : \"file:///project/src/main.rs\" , \"name\" : \"main.rs\" , \"description\" : \"Primary application entry point\" , \"mimeType\" : \"text/x-rust\" , \"annotations\" : { \"audience\" : [ \"assistant\" ], \"priority\" : 0.9 } } Resource links support the same Resource annotations as regular resources to help clients understand how to use them. Resource links returned by tools are not guaranteed to appear in the results of a resources/list request. â€‹ Embedded Resources Resources MAY be embedded to provide additional context or data using a suitable URI scheme . Servers that use embedded resources SHOULD implement the resources capability: Copy { \"type\" : \"resource\" , \"resource\" : { \"uri\" : \"file:///project/src/main.rs\" , \"mimeType\" : \"text/x-rust\" , \"text\" : \"fn main() { \\n println!( \\\" Hello world! \\\" ); \\n }\" , \"annotations\" : { \"audience\" : [ \"user\" , \"assistant\" ], \"priority\" : 0.7 , \"lastModified\" : \"2025-05-03T14:30:00Z\" } } } Embedded resources support the same Resource annotations as regular resources to help clients understand how to use them. â€‹ Structured Content Structured content is returned as a JSON object in the structuredContent field of a result. For backwards compatibility, a tool that returns structured content SHOULD also return the serialized JSON in a TextContent block. â€‹ Output Schema Tools may also provide an output schema for validation of structured results. If an output schema is provided: Servers MUST provide structured results that conform to this schema. Clients SHOULD validate structured results against this schema. Example tool with output schema: Copy { \"name\" : \"get_weather_data\" , \"title\" : \"Weather Data Retriever\" , \"description\" : \"Get current weather data for a location\" , \"inputSchema\" : { \"type\" : \"object\" , \"properties\" : { \"location\" : { \"type\" : \"string\" , \"description\" : \"City name or zip code\" } }, \"required\" : [ \"location\" ] }, \"outputSchema\" : { \"type\" : \"object\" , \"properties\" : { \"temperature\" : { \"type\" : \"number\" , \"description\" : \"Temperature in celsius\" }, \"conditions\" : { \"type\" : \"string\" , \"description\" : \"Weather conditions description\" }, \"humidity\" : { \"type\" : \"number\" , \"description\" : \"Humidity percentage\" } }, \"required\" : [ \"temperature\" , \"conditions\" , \"humidity\" ] } } Example valid response for this tool: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 5 , \"result\" : { \"content\" : [ { \"type\" : \"text\" , \"text\" : \"{ \\\" temperature \\\" : 22.5, \\\" conditions \\\" : \\\" Partly cloudy \\\" , \\\" humidity \\\" : 65}\" } ], \"structuredContent\" : { \"temperature\" : 22.5 , \"conditions\" : \"Partly cloudy\" , \"humidity\" : 65 } } } Providing an output schema helps clients and LLMs understand and properly handle structured tool outputs by: Enabling strict schema validation of responses Providing type information for better integration with programming languages Guiding clients and LLMs to properly parse and utilize the returned data Supporting better documentation and developer experience â€‹ Error Handling Tools use two error reporting mechanisms: Protocol Errors : Standard JSON-RPC errors for issues like: Unknown tools Invalid arguments Server errors Tool Execution Errors : Reported in tool results with isError: true : API failures Invalid input data Business logic errors Example protocol error: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 3 , \"error\" : { \"code\" : -32602 , \"message\" : \"Unknown tool: invalid_tool_name\" } } Example tool execution error: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 4 , \"result\" : { \"content\" : [ { \"type\" : \"text\" , \"text\" : \"Failed to fetch weather data: API rate limit exceeded\" } ], \"isError\" : true } } â€‹ Security Considerations Servers MUST : Validate all tool inputs Implement proper access controls Rate limit tool invocations Sanitize tool outputs Clients SHOULD : Prompt for user confirmation on sensitive operations Show tool inputs to the user before calling the server, to avoid malicious or accidental data exfiltration Validate tool results before passing to LLM Implement timeouts for tool calls Log tool usage for audit purposes Was this page helpful? Yes No Resources Completion âŒ˜ I github","headings":[]}
{"id":"concepts-resources-chunk-0","pageId":"concepts-resources","url":"https://modelcontextprotocol.io/docs/concepts/resources","title":"Resources - Model Context Protocol","text":"Resources - Model Context Protocol Skip to main content Model Context Protocol home page Version 2025-06-18 (latest) Search... âŒ˜ K Blog GitHub Search... Navigation Server Features Resources Documentation Specification Community About MCP Specification Key Changes Architecture Base Protocol Overview Lifecycle Transports Authorization Security Best Practices Utilities Client Features Roots Sampling Elicitation Server Features Overview Prompts Resources Tools Utilities Schema Reference On this page User Interaction Model Capabilities Protocol Messages Listing Resources Reading Resources Resource Templates List Changed Notification Subscriptions Message Flow Data Types Resource Resource Contents Text Content Binary Content Annotations Common URI Schemes https:// file:// git:// Custom URI Schemes Error Handling Security Considerations Server Features Resources Copy page Copy page Protocol Revision : 2025-06-18 The Model Context Protocol (MCP) provides a standardized way for servers to expose resources to clients. Resources allow servers to share data that provides context to language models, such as files, database schemas, or application-specific information. Each resource is uniquely identified by a URI . â€‹ User Interaction Model Resources in MCP are designed to be application-driven , with host applications determining how to incorporate context based on their needs. For example, applications could: Expose resources through UI elements for explicit selection, in a tree or list view Allow the user to search through and filter available resources Implement automatic context inclusion, based on heuristics or the AI modelâ€™s selection However, implementations are free to expose resources through any interface pattern that suits their needsâ€”the protocol itself does not mandate any specific user interaction model. â€‹ Capabilities Servers that support resources MUST declare the resources capability: Copy { \"capabilities\" : { \"resources\" : { \"subscribe\" : true , \"listChanged\" : true } } } The capability supports two optional features: subscribe : whether the client can subscribe to be notified of changes to individual resources. listChanged : whether the server will emit notifications when the list of available resources changes. Both subscribe and listChanged are optionalâ€”servers can support neither, either, or both: Copy { \"capabilities\" : { \"resources\" : {} // Neither feature supported } } Copy { \"capabilities\" : { \"resources\" : { \"subscribe\" : true // Only subscriptions supported } } } Copy { \"capabilities\" : { \"resources\" : { \"listChanged\" : true // Only list change notifications supported } } } â€‹ Protocol Messages â€‹ Listing Resources To discover available resources, clients send a resources/list request. This operation supports pagination . Request: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 1 , \"method\" : \"resources/list\" , \"params\" : { \"cursor\" : \"optional-cursor-value\" } } Response: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 1 , \"result\" : { \"resources\" : [ { \"uri\" : \"file:///project/src/main.rs\" , \"name\" : \"main.rs\" , \"title\" : \"Rust Software Application Main File\" , \"description\" : \"Primary application entry point\" , \"mimeType\" : \"text/x-rust\" } ], \"nextCursor\" : \"next-page-cursor\" } } â€‹ Reading Resources To retrieve resource contents, clients send a resources/read request: Request: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 2 , \"method\" : \"resources/read\" , \"params\" : { \"uri\" : \"file:///project/src/main.rs\" } } Response: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 2 , \"result\" : { \"contents\" : [ { \"uri\" : \"file:///project/src/main.rs\" , \"mimeType\" : \"text/x-rust\" , \"text\" : \"fn main() { \\n println!( \\\" Hello world! \\\" ); \\n }\" } ] } } â€‹ Resource Templates Resource templates allow servers to expose parameterized resources using URI templates . Arguments may be auto-completed through the completion API . Request: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 3 , \"method\" : \"resources/templates/list\" } Response: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 3 , \"result\" : { \"resourceTemplates\" : [ { \"uriTemplate\" : \"file:///{path}\" , \"name\" : \"Project Files\" , \"title\" : \"ðŸ“ Project Files\" , \"description\" : \"Access files in the project directory\" , \"mimeType\" : \"application/octet-stream\" } ] } } â€‹ List Changed Notification When the list of available resources changes, servers that declared the listChanged capability SHOULD send a notification: Copy { \"jsonrpc\" : \"2.0\" , \"method\" : \"notifications/resources/list_changed\" } â€‹ Subscriptions The protocol supports optional subscriptions to resource changes. Clients can subscribe to specific resources and receive notifications when they change: Subscribe Request: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 4 , \"method\" : \"resources/subscribe\" , \"params\" : { \"uri\" : \"file:///project/src/main.rs\" } } Update Notification: Copy { \"jsonrpc\" : \"2.0\" , \"method\" : \"notifications/resources/updated\" , \"params\" : { \"uri\" : \"file:///project/src/main.rs\" } } â€‹ Message Flow â€‹ Data Types â€‹ Resource A resource definition includes: uri : Unique identifier for the resource name : The name of the resource. title : Optional human-readable name of the resource for display purposes. description : Optional description mimeType : Optional MIME type size : Optional size in bytes â€‹ Resource Contents Resources can contain either text or binary data: â€‹ Text Content Copy { \"uri\" : \"file:///example.txt\" , \"mimeType\" : \"text/plain\" , \"text\" : \"Resource content\" } â€‹ Binary Content Copy { \"uri\" : \"file:///example.png\" , \"mimeType\" : \"image/png\" , \"blob\" : \"base64-encoded-data\" } â€‹ Annotations Resources, resource templates and content blocks support optional annotations that provide hints to clients about how to use or display the resource: audience : An array indicating the intended audience(s) for this resource. Valid values are \"user\" and \"assistant\" . For example, [\"user\", \"assistant\"] indicates content useful for both. priority : A number from 0.0 to 1.0 indicating the importance of this resource. A value of 1 means â€œmost importantâ€ (effectively required), while 0 means â€œleast importantâ€ (entirely optional). lastModified : An ISO 8601 formatted timestamp indicating when the resource was last modified (e.g., \"2025-01-12T15:00:58Z\" ). Example resource with annotations: Copy { \"uri\" : \"file:///project/README.md\" , \"name\" : \"README.md\" , \"title\" : \"Project Documentation\" , \"mimeType\" : \"text/markdown\" , \"annotations\" : { \"audience\" : [ \"user\" ], \"priority\" : 0.8 , \"lastModified\" : \"2025-01-12T15:00:58Z\" } } Clients can use these annotations to: Filter resources based on their intended audience Prioritize which resources to include in context Display modification times or sort by recency â€‹ Common URI Schemes The protocol defines several standard URI schemes. This list not exhaustiveâ€”implementations are always free to use additional, custom URI schemes. â€‹ https:// Used to represent a resource available on the web. Servers SHOULD use this scheme only when the client is able to fetch and load the resource directly from the web on its ownâ€”that is, it doesnâ€™t need to read the resource via the MCP server. For other use cases, servers SHOULD prefer to use another URI scheme, or define a custom one, even if the server will itself be downloading resource contents over the internet. â€‹ file:// Used to identify resources that behave like a filesystem. However, the resources do not need to map to an actual physical filesystem. MCP servers MAY identify file:// resources with an XDG MIME type , like inode/directory , to represent non-regular files (such as directories) that donâ€™t otherwise have a standard MIME type. â€‹ git:// Git version control integration. â€‹ Custom URI Schemes Custom URI schemes MUST be in accordance with RFC3986 , taking the above guidance in to account. â€‹ Error Handling Servers SHOULD return standard JSON-RPC errors for common failure cases: Resource not found: -32002 Internal errors: -32603 Example error: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 5 , \"error\" : { \"code\" : -32002 , \"message\" : \"Resource not found\" , \"data\" : { \"uri\" : \"file:///nonexistent.txt\" } } } â€‹ Security Considerations Servers MUST validate all resource URIs Access controls SHOULD be implemented for sensitive resources Binary data MUST be properly encoded Resource permissions SHOULD be checked before operations Was this page helpful? Yes No Prompts Tools âŒ˜ I github","headings":[]}
{"id":"concepts-prompts-chunk-0","pageId":"concepts-prompts","url":"https://modelcontextprotocol.io/docs/concepts/prompts","title":"Prompts - Model Context Protocol","text":"Prompts - Model Context Protocol Skip to main content Model Context Protocol home page Version 2025-06-18 (latest) Search... âŒ˜ K Blog GitHub Search... Navigation Server Features Prompts Documentation Specification Community About MCP Specification Key Changes Architecture Base Protocol Overview Lifecycle Transports Authorization Security Best Practices Utilities Client Features Roots Sampling Elicitation Server Features Overview Prompts Resources Tools Utilities Schema Reference On this page User Interaction Model Capabilities Protocol Messages Listing Prompts Getting a Prompt List Changed Notification Message Flow Data Types Prompt PromptMessage Text Content Image Content Audio Content Embedded Resources Error Handling Implementation Considerations Security Server Features Prompts Copy page Copy page Protocol Revision : 2025-06-18 The Model Context Protocol (MCP) provides a standardized way for servers to expose prompt templates to clients. Prompts allow servers to provide structured messages and instructions for interacting with language models. Clients can discover available prompts, retrieve their contents, and provide arguments to customize them. â€‹ User Interaction Model Prompts are designed to be user-controlled , meaning they are exposed from servers to clients with the intention of the user being able to explicitly select them for use. Typically, prompts would be triggered through user-initiated commands in the user interface, which allows users to naturally discover and invoke available prompts. For example, as slash commands: However, implementors are free to expose prompts through any interface pattern that suits their needsâ€”the protocol itself does not mandate any specific user interaction model. â€‹ Capabilities Servers that support prompts MUST declare the prompts capability during initialization : Copy { \"capabilities\" : { \"prompts\" : { \"listChanged\" : true } } } listChanged indicates whether the server will emit notifications when the list of available prompts changes. â€‹ Protocol Messages â€‹ Listing Prompts To retrieve available prompts, clients send a prompts/list request. This operation supports pagination . Request: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 1 , \"method\" : \"prompts/list\" , \"params\" : { \"cursor\" : \"optional-cursor-value\" } } Response: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 1 , \"result\" : { \"prompts\" : [ { \"name\" : \"code_review\" , \"title\" : \"Request Code Review\" , \"description\" : \"Asks the LLM to analyze code quality and suggest improvements\" , \"arguments\" : [ { \"name\" : \"code\" , \"description\" : \"The code to review\" , \"required\" : true } ] } ], \"nextCursor\" : \"next-page-cursor\" } } â€‹ Getting a Prompt To retrieve a specific prompt, clients send a prompts/get request. Arguments may be auto-completed through the completion API . Request: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 2 , \"method\" : \"prompts/get\" , \"params\" : { \"name\" : \"code_review\" , \"arguments\" : { \"code\" : \"def hello(): \\n print(&#x27;world&#x27;)\" } } } Response: Copy { \"jsonrpc\" : \"2.0\" , \"id\" : 2 , \"result\" : { \"description\" : \"Code review prompt\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : { \"type\" : \"text\" , \"text\" : \"Please review this Python code: \\n def hello(): \\n print(&#x27;world&#x27;)\" } } ] } } â€‹ List Changed Notification When the list of available prompts changes, servers that declared the listChanged capability SHOULD send a notification: Copy { \"jsonrpc\" : \"2.0\" , \"method\" : \"notifications/prompts/list_changed\" } â€‹ Message Flow â€‹ Data Types â€‹ Prompt A prompt definition includes: name : Unique identifier for the prompt title : Optional human-readable name of the prompt for display purposes. description : Optional human-readable description arguments : Optional list of arguments for customization â€‹ PromptMessage Messages in a prompt can contain: role : Either â€œuserâ€ or â€œassistantâ€ to indicate the speaker content : One of the following content types: All content types in prompt messages support optional annotations for metadata about audience, priority, and modification times. â€‹ Text Content Text content represents plain text messages: Copy { \"type\" : \"text\" , \"text\" : \"The text content of the message\" } This is the most common content type used for natural language interactions. â€‹ Image Content Image content allows including visual information in messages: Copy { \"type\" : \"image\" , \"data\" : \"base64-encoded-image-data\" , \"mimeType\" : \"image/png\" } The image data MUST be base64-encoded and include a valid MIME type. This enables multi-modal interactions where visual context is important. â€‹ Audio Content Audio content allows including audio information in messages: Copy { \"type\" : \"audio\" , \"data\" : \"base64-encoded-audio-data\" , \"mimeType\" : \"audio/wav\" } The audio data MUST be base64-encoded and include a valid MIME type. This enables multi-modal interactions where audio context is important. â€‹ Embedded Resources Embedded resources allow referencing server-side resources directly in messages: Copy { \"type\" : \"resource\" , \"resource\" : { \"uri\" : \"resource://example\" , \"mimeType\" : \"text/plain\" , \"text\" : \"Resource content\" } } Resources can contain either text or binary (blob) data and MUST include: A valid resource URI The appropriate MIME type Either text content or base64-encoded blob data Embedded resources enable prompts to seamlessly incorporate server-managed content like documentation, code samples, or other reference materials directly into the conversation flow. â€‹ Error Handling Servers SHOULD return standard JSON-RPC errors for common failure cases: Invalid prompt name: -32602 (Invalid params) Missing required arguments: -32602 (Invalid params) Internal errors: -32603 (Internal error) â€‹ Implementation Considerations Servers SHOULD validate prompt arguments before processing Clients SHOULD handle pagination for large prompt lists Both parties SHOULD respect capability negotiation â€‹ Security Implementations MUST carefully validate all prompt inputs and outputs to prevent injection attacks or unauthorized access to resources. Was this page helpful? Yes No Overview Resources âŒ˜ I github","headings":[]}
{"id":"concepts-server-info-chunk-0","pageId":"concepts-server-info","url":"https://modelcontextprotocol.io/docs/concepts/server","title":"Page Not Found","text":"Page Not Found Skip to main content Model Context Protocol home page Search... âŒ˜ K Blog GitHub Search... Navigation Page Not Found Documentation Specification Community About MCP Get started What is MCP? About MCP Architecture Servers Clients Versioning Develop with MCP Connect to local MCP servers Connect to remote MCP Servers Build an MCP server Build an MCP client SDKs Security Developer tools MCP Inspector 404 Page Not Found We couldn&#x27;t find the page. Maybe you were looking for one of these pages below? Build an MCP server Understanding MCP servers Example Servers âŒ˜ I","headings":[]}
{"id":"concepts-lifecycle-chunk-0","pageId":"concepts-lifecycle","url":"https://modelcontextprotocol.io/docs/concepts/lifecycle","title":"Page Not Found","text":"Page Not Found Skip to main content Model Context Protocol home page Search... âŒ˜ K Blog GitHub Search... Navigation Page Not Found Documentation Specification Community About MCP Get started What is MCP? About MCP Architecture Servers Clients Versioning Develop with MCP Connect to local MCP servers Connect to remote MCP Servers Build an MCP server Build an MCP client SDKs Security Developer tools MCP Inspector 404 Page Not Found We couldn&#x27;t find the page. Maybe you were looking for one of these pages below? Lifecycle Architecture overview âŒ˜ I","headings":[]}
{"id":"concepts-authentication-chunk-0","pageId":"concepts-authentication","url":"https://modelcontextprotocol.io/docs/concepts/authentication","title":"Page Not Found","text":"Page Not Found Skip to main content Model Context Protocol home page Search... âŒ˜ K Blog GitHub Search... Navigation Page Not Found Documentation Specification Community About MCP Get started What is MCP? About MCP Architecture Servers Clients Versioning Develop with MCP Connect to local MCP servers Connect to remote MCP Servers Build an MCP server Build an MCP client SDKs Security Developer tools MCP Inspector 404 Page Not Found We couldn&#x27;t find the page. Maybe you were looking for one of these pages below? Build an MCP server Authorization Architecture overview âŒ˜ I","headings":[]}
{"id":"fastmcp-punkpeye-readme-chunk-0","pageId":"fastmcp-punkpeye-readme","url":"https://github.com/punkpeye/fastmcp/blob/main/README.md","title":"fastmcp - README","text":"# FastMCP A TypeScript framework for building [MCP](https://glama.ai/mcp) servers capable of handling client sessions. > [!NOTE] > > For a Python implementation, see [FastMCP](https://github.com/jlowin/fastmcp). ## Features - Simple Tool, Resource, Prompt definition - [Authentication](#authentication) - [Passing headers through context](#passing-headers-through-context) - [Session ID and Request ID tracking](#session-id-and-request-id-tracking) - [Sessions](#sessions) - [Image content](#returning-an-image) - [Audio content](#returning-an-audio) - [Embedded](#embedded-resources) - [Logging](#logging) - [Error handling](#errors) - [HTTP Streaming](#http-streaming) (with SSE compatibility) - [Stateless mode](#stateless-mode) for serverless deployments - CORS (enabled by default) - [Progress notifications](#progress) - [Streaming output](#streaming-output) - [Typed server events](#typed-server-events) - [Prompt argument auto-completion](#prompt-argument-auto-completion) - [Sampling](#requestsampling) - [Configurable ping behavior](#configurable-ping-behavior) - [Health-check endpoint](#health-check-endpoint) - [Roots](#roots-management) - CLI for [testing](#test-with-mcp-cli) and [debugging](#inspect-with-mcp-inspector) ## When to use FastMCP over the official SDK? FastMCP is built on top of the official SDK. The official SDK provides foundational blocks for building MCPs, but leaves many implementation details to you: - [Initiating and configuring](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L664-L744) all the server components - [Handling of connections](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L760-L850) - [Handling of tools](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L1303-L1498) - [Handling of responses](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L989-L1060) - [Handling of resources](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L1151-L1242) - Adding [prompts](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L760-L850), [resources](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L960-L962), [resource templates](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L964-L987) - Embedding [resources](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L1569-L1643), [image](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L51-L111) and [audio](https://github.com/punkpeye/fastmcp/blob/06c2af7a3d7e3d8c638deac1964ce269ce8e518b/src/FastMCP.ts#L113-L173) content blocks FastMCP eliminates this complexity by providing an opinionated framework that: - Handles all the boilerplate automatically - Provides simple, intuitive APIs for common tasks - Includes built-in best practices and error handling - Lets you focus on your MCP's core functionality **When to choose FastMCP:** You want to build MCP servers quickly without dealing with low-level implementation details. **When to use the official SDK:** You need maximum control or have specific architectural requirements. In this case, we encourage referencing FastMCP's implementation to avoid common pitfalls. ## Installation ```bash npm install fastmcp ``` ## Quickstart > [!NOTE] > > There are many real-world examples of using FastMCP in the wild. See the [Showcase](#showcase) for examples. ```ts import { FastMCP } from \"fastmcp\"; import { z } from \"zod\"; // Or any validation library that supports Standard Schema const server = new FastMCP({ name: \"My Server\", version: \"1.0.0\", }); server.addTool({ name: \"add\", description: \"Add two numbers\", parameters: z.object({ a: z.number(), b: z.number(), }), execute: async (args) => { return String(args.a + args.b); }, }); server.start({ transportType: \"stdio\", }); ``` _That's it!_ You have a working MCP server. You can test the server in terminal with: ```bash git clone https://github.com/punkpeye/fastmcp.git cd fastmcp pnpm install pnpm build # Test the addition server example using CLI: npx fastmcp dev src/examples/addition.ts # Test the addition server example using MCP Inspector: npx fastmcp inspect src/examples/addition.ts ``` If you are looking for a boilerplate repository to build your own MCP server, check out [fastmcp-boilerplate](https://github.com/punkpeye/fastmcp-boilerplate). ### Remote Server Options FastMCP supports multiple transport options for remote communication, allowing an MCP hosted on a remote machine to be accessed over the network. #### HTTP Streaming [HTTP streaming](https://www.cloudflare.com/learning/video/what-is-http-live-streaming/) provides a more efficient alternative to SSE in environments that support it, with potentially better performance for larger payloads. You can run the server with HTTP streaming support: ```ts server.start({ transportType: \"httpStream\", httpStream: { port: 8080, }, }); ``` This will start the server and listen for HTTP streaming connections on `http://localhost:8080/mcp`. > **Note:** You can also customize the endpoint path using the `httpStream.endpoint` option (default is `/mcp`). > **Note:** This also starts an SSE server on `http://localhost:8080/sse`. You can connect to these servers using the appropriate client transport. For HTTP streaming connections: ```ts import { StreamableHTTPClientTransport } from \"@modelcontextprotocol/sdk/client/streamableHttp.js\"; const client = new Client( { name: \"example-client\", version: \"1.0.0\", }, { capabilities: {}, }, ); const transport = new StreamableHTTPClientTransport( new URL(`http://localhost:8080/mcp`), ); await client.connect(transport); ``` For SSE connections: ```ts import { SSEClientTransport } from \"@modelcontextprotocol/sdk/client/sse.js\"; const client = new Client( { name: \"example-client\", version: \"1.0.0\", }, { capabilities: {}, }, ); const transport = new SSEClientTransport(new URL(`http://localhost:8080/sse`)); await client.connect(transport); ``` #### Stateless Mode FastMCP supports stateless operation for HTTP streaming, where each request is handled independently without maintaining persistent sessions. This is ideal for serverless environments, load-balanced deployments, or when session state isn't required. In stateless mode: - No sessions are tracked on the server - Each request creates a temporary session that's discarded after the response - Reduced memory usage and better scalability - Perfect for stateless deployment environments You can enable stateless mode by adding the `stateless: true` option: ```ts server.start({ transportType: \"httpStream\", httpStream: { port: 8080, stateless: true, }, }); ``` > **Note:** Stateless mode is only available with HTTP streaming transport. Features that depend on persistent sessions (like session-specific state) will not be available in stateless mode. You can also enable stateless mode using CLI arguments or environment variables: ```bash # Via CLI argument npx fastmcp dev src/server.ts --transport http-stream --port 8080 --stateless true # Via environment variable FASTMCP_STATELESS=true npx fastmcp dev src/server.ts ``` The `/ready` health check endpoint will indicate when the server is running in stateless mode: ```json { \"mode\": \"stateless\", \"ready\": 1, \"status\": \"ready\", \"total\": 1 } ``` ## Core Concepts ### Tools [Tools](https://modelcontextprotocol.io/docs/concepts/tools) in MCP allow servers to expose executable functions that can be invoked by clients and used by LLMs to perform actions. FastMCP uses the [Standard Schema](https://standardschema.dev) specification for defining tool parameters. This allows you to use your preferred schema validation library (like Zod, ArkType, or Valibot) as long as it implements the spec. **Zod Example:** ```typescript import { z } from \"zod\"; server.addTool({ name: \"fetch-zod\", description: \"Fetch the content of a url (using Zod)\", parameters: z.object({ url: z.string(), }), execute: async (args) => { return await fetchWebpageContent(args.url); }, }); ``` **ArkType Example:** ```typescript import { type } from \"arktype\"; server.addTool({ name: \"fetch-arktype\", description: \"Fetch the content of a url (using ArkType)\", parameters: type({ url: \"string\", }), execute: async (args) => { return await fetchWebpageContent(args.url); }, }); ``` **Valibot Example:** Valibot requires the peer dependency @valibot/to-json-schema. ```typescript import * as v from \"valibot\"; server.addTool({ name: \"fetch-valibot\", description: \"Fetch the content of a url (using Valibot)\", parameters: v.object({ url: v.string(), }), execute: async (args) => { return await fetchWebpageContent(args.url); }, }); ``` #### Tools Without Parameters When creating tools that don't require parameters, you have two options: 1. Omit the parameters property entirely: ```typescript server.addTool({ name: \"sayHello\", description: \"Say hello\", // No parameters property execute: async () => { return \"Hello, world!\"; }, }); ``` 2. Explicitly define empty parameters: ```typescript import { z } from \"zod\"; server.addTool({ name: \"sayHello\", description: \"Say hello\", parameters: z.object({}), // Empty object execute: async () => { return \"Hello, world!\"; }, }); ``` > [!NOTE] > > Both approaches are fully compatible with all MCP clients, including Cursor. FastMCP automatically generates the proper schema in both cases. #### Tool Authorization You can control which tools are available to authenticated users by adding an optional `canAccess` function to a tool's definition. This function receives the authentication context and should return `true` if the user is allowed to access the tool. ```typescript server.addTool({ name: \"admin-tool\", description: \"An admin-only tool\", canAccess: (auth) => auth?.role === \"admin\", execute: async () => \"Welcome, admin!\", }); ``` #### Returning a string `execute` can return a string: ```js server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args) => { return \"Hello, world!\"; }, }); ``` The latter is equivalent to: ```js server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args) => { return { content: [ { type: \"text\", text: \"Hello, world!\", }, ], }; }, }); ``` #### Returning a list If you want to return a list of messages, you can return an object with a `content` property: ```js server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args) => { return { content: [ { type: \"text\", text: \"First message\" }, { type: \"text\", text: \"Second message\" }, ], }; }, }); ``` #### Returning an image Use the `imageContent` to create a content object for an image: ```js import { imageContent } from \"fastmcp\"; server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args) => { return imageContent({ url: \"https://example.com/image.png\", }); // or... // return imageContent({ // path: \"/path/to/image.png\", // }); // or... // return imageContent({ // buffer: Buffer.from(\"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\", \"base64\"), // }); // or... // return { // content: [ // await imageContent(...) // ], // }; }, }); ``` The `imageContent` function takes the following options: - `url`: The URL of the image. - `path`: The path to the image file. - `buffer`: The image data as a buffer. Only one of `url`, `path`, or `buffer` must be specified. The above example is equivalent to: ```js server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args) => { return { content: [ { type: \"image\", data: \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\", mimeType: \"image/png\", }, ], }; }, }); ``` #### Configurable Ping Behavior FastMCP includes a configurable ping mechanism to maintain connection health. The ping behavior can be customized through server options: ```ts const server = new FastMCP({ name: \"My Server\", version: \"1.0.0\", ping: { // Explicitly enable or disable pings (defaults vary by transport) enabled: true, // Configure ping interval in milliseconds (default: 5000ms) intervalMs: 10000, // Set log level for ping-related messages (default: 'debug') logLevel: \"debug\", }, }); ``` By default, ping behavior is optimized for each transport type: - Enabled for SSE and HTTP streaming connections (which benefit from keep-alive) - Disabled for `stdio` connections (where pings are typically unnecessary) This configurable approach helps reduce log verbosity and optimize performance for different usage scenarios. ### Health-check Endpoint When you run FastMCP with the `httpStream` transport you can optionally expose a simple HTTP endpoint that returns a plain-text response useful for load-balancer or container orchestration liveness checks. Enable (or customise) the endpoint via the `health` key in the server options: ```ts const server = new FastMCP({ name: \"My Server\", version: \"1.0.0\", health: { // Enable / disable (default: true) enabled: true, // Body returned by the endpoint (default: 'ok') message: \"healthy\", // Path that should respond (default: '/health') path: \"/healthz\", // HTTP status code to return (default: 200) status: 200, }, }); await server.start({ transportType: \"httpStream\", httpStream: { port: 8080 }, }); ``` Now a request to `http://localhost:8080/healthz` will return: ``` HTTP/1.1 200 OK content-type: text/plain healthy ``` The endpoint is ignored when the server is started with the `stdio` transport. #### Roots Management FastMCP supports [Roots](https://modelcontextprotocol.io/docs/concepts/roots) - Feature that allows clients to provide a set of filesystem-like root locations that can be listed and dynamically updated. The Roots feature can be configured or disabled in server options: ```ts const server = new FastMCP({ name: \"My Server\", version: \"1.0.0\", roots: { // Set to false to explicitly disable roots support enabled: false, // By default, roots support is enabled (true) }, }); ``` This provides the following benefits: - Better compatibility with different clients that may not support Roots - Reduced error logs when connecting to clients that don't implement roots capability - More explicit control over MCP server capabilities - Graceful degradation when roots functionality isn't available You can listen for root changes in your server: ```ts server.on(\"connect\", (event) => { const session = event.session; // Access the current roots console.log(\"Initial roots:\", session.roots); // Listen for changes to the roots session.on(\"rootsChanged\", (event) => { console.log(\"Roots changed:\", event.roots); }); }); ``` When a client doesn't support roots or when roots functionality is explicitly disabled, these operations will gracefully handle the situation without throwing errors. ### Returning an audio Use the `audioContent` to create a content object for an audio: ```js import { audioContent } from \"fastmcp\"; server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args) => { return audioContent({ url: \"https://example.com/audio.mp3\", }); // or... // return audioContent({ // path: \"/path/to/audio.mp3\", // }); // or... // return audioContent({ // buffer: Buffer.from(\"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\", \"base64\"), // }); // or... // return { // content: [ // await audioContent(...) // ], // }; }, }); ``` The `audioContent` function takes the following options: - `url`: The URL of the audio. - `path`: The path to the audio file. - `buffer`: The audio data as a buffer. Only one of `url`, `path`, or `buffer` must be specified. The above example is equivalent to: ```js server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args) => { return { content: [ { type: \"audio\", data: \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\", mimeType: \"audio/mpeg\", }, ], }; }, }); ``` #### Return combination type You can combine various types in this way and send them back to AI ```js server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args) => { return { content: [ { type: \"text\", text: \"Hello, world!\", }, { type: \"image\", data: \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\", mimeType: \"image/png\", }, { type: \"audio\", data: \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\", mimeType: \"audio/mpeg\", }, ], }; }, // or... // execute: async (args) => { // const imgContent = await imageContent({ // url: \"https://example.com/image.png\", // }); // const audContent = await audioContent({ // url: \"https://example.com/audio.mp3\", // }); // return { // content: [ // { // type: \"text\", // text: \"Hello, world!\", // }, // imgContent, // audContent, // ], // }; // }, }); ``` #### Custom Logger FastMCP allows you to provide a custom logger implementation to control how the server logs messages. This is useful for integrating with existing logging infrastructure or customizing log formatting. ```ts import { FastMCP, Logger } from \"fastmcp\"; class CustomLogger implements Logger { debug(...args: unknown[]): void { console.log(\"[DEBUG]\", new Date().toISOString(), ...args); } error(...args: unknown[]): void { console.error(\"[ERROR]\", new Date().toISOString(), ...args); } info(...args: unknown[]): void { console.info(\"[INFO]\", new Date().toISOString(), ...args); } log(...args: unknown[]): void { console.log(\"[LOG]\", new Date().toISOString(), ...args); } warn(...args: unknown[]): void { console.warn(\"[WARN]\", new Date().toISOString(), ...args); } } const server = new FastMCP({ name: \"My Server\", version: \"1.0.0\", logger: new CustomLogger(), }); ``` See `src/examples/custom-logger.ts` for examples with Winston, Pino, and file-based logging. #### Logging Tools can log messages to the client using the `log` object in the context object: ```js server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args, { log }) => { log.info(\"Downloading file...\", { url, }); // ... log.info(\"Downloaded file\"); return \"done\"; }, }); ``` The `log` object has the following methods: - `debug(message: string, data?: SerializableValue)` - `error(message: string, data?: SerializableValue)` - `info(message: string, data?: SerializableValue)` - `warn(message: string, data?: SerializableValue)` #### Errors The errors that are meant to be shown to the user should be thrown as `UserError` instances: ```js import { UserError } from \"fastmcp\"; server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args) => { if (args.url.startsWith(\"https://example.com\")) { throw new UserError(\"This URL is not allowed\"); } return \"done\"; }, }); ``` #### Progress Tools can report progress by calling `reportProgress` in the context object: ```js server.addTool({ name: \"download\", description: \"Download a file\", parameters: z.object({ url: z.string(), }), execute: async (args, { reportProgress }) => { await reportProgress({ progress: 0, total: 100, }); // ... await reportProgress({ progress: 100, total: 100, }); return \"done\"; }, }); ``` #### Streaming Output FastMCP supports streaming partial results from tools while they're still executing, enabling responsive UIs and real-time feedback. This is particularly useful for: - Long-running operations that generate content incrementally - Progressive generation of text, images, or other media - Operations where users benefit from seeing immediate partial results To enable streaming for a tool, add the `streamingHint` annotation and use the `streamContent` method: ```js server.addTool({ name: \"generateText\", description: \"Generate text incrementally\", parameters: z.object({ prompt: z.string(), }), annotations: { streamingHint: true, // Signals this tool uses streaming readOnlyHint: true, }, execute: async (args, { streamContent }) => { // Send initial content immediately await streamContent({ type: \"text\", text: \"Starting generation...\\n\" }); // Simulate incremental content generation const words = \"The quick brown fox jumps over the lazy dog.\".split(\" \"); for (const word of words) { await streamContent({ type: \"text\", text: word + \" \" }); await new Promise((resolve) => setTimeout(resolve, 300)); // Simulate delay } // When using streamContent, you can: // 1. Return void (if all content was streamed) // 2. Return a final result (which will be appended to streamed content) // Option 1: All content was streamed, so return void return; // Option 2: Return final content that will be appended // return \"Generation complete!\"; }, }); ``` Streaming works with all content types (text, image, audio) and can be combined with progress reporting: ```js server.addTool({ name: \"processData\", description: \"Process data with streaming updates\", parameters: z.object({ datasetSize: z.number(), }), annotations: { streamingHint: true, }, execute: async (args, { streamContent, reportProgress }) => { const total = args.datasetSize; for (let i = 0; i setTimeout(resolve, 50)); } return \"Processing complete!\"; }, }); ``` #### Tool Annotations As of the MCP Specification (2025-03-26), tools can include annotations that provide richer context and control by adding metadata about a tool's behavior: ```typescript server.addTool({ name: \"fetch-content\", description: \"Fetch content from a URL\", parameters: z.object({ url: z.string(), }), annotations: { title: \"Web Content Fetcher\", // Human-readable title for UI display readOnlyHint: true, // Tool doesn't modify its environment openWorldHint: true, // Tool interacts with external entities }, execute: async (args) => { return await fetchWebpageContent(args.url); }, }); ``` The available annotations are: | Annotation | Type | Default | Description | | :---------------- | :------ | :------ | :----------------------------------------------------------------------------------------------------------------------------------- | | `title` | string | - | A human-readable title for the tool, useful for UI display | | `readOnlyHint` | boolean | `false` | If true, indicates the tool does not modify its environment | | `destructiveHint` | boolean | `true` | If true, the tool may perform destructive updates (only meaningful when `readOnlyHint` is false) | | `idempotentHint` | boolean | `false` | If true, calling the tool repeatedly with the same arguments has no additional effect (only meaningful when `readOnlyHint` is false) | | `openWorldHint` | boolean | `true` | If true, the tool may interact with an \"open world\" of external entities | These annotations help clients and LLMs better understand how to use the tools and what to expect when calling them. ### Resources [Resources](https://modelcontextprotocol.io/docs/concepts/resources) represent any kind of data that an MCP server wants to make available to clients. This can include: - File contents - Screenshots and images - Log files - And more Each resource is identified by a unique URI and can contain either text or binary data. ```ts server.addResource({ uri: \"file:///logs/app.log\", name: \"Application Logs\", mimeType: \"text/plain\", async load() { return { text: await readLogFile(), }; }, }); ``` > [!NOTE] > > `load` can return multiple resources. This could be used, for example, to return a list of files inside a directory when the directory is read. > > ```ts > async load() { > return [ > { > text: \"First file content\", > }, > { > text: \"Second file content\", > }, > ]; > } > ``` You can also return binary contents in `load`: ```ts async load() { return { blob: 'base64-encoded-data' }; } ``` ### Resource templates You can also define resource templates: ```ts server.addResourceTemplate({ uriTemplate: \"file:///logs/{name}.log\", name: \"Application Logs\", mimeType: \"text/plain\", arguments: [ { name: \"name\", description: \"Name of the log\", required: true, }, ], async load({ name }) { return { text: `Example log content for ${name}`, }; }, }); ``` #### Resource template argument auto-completion Provide `complete` functions for resource template arguments to enable automatic completion: ```ts server.addResourceTemplate({ uriTemplate: \"file:///logs/{name}.log\", name: \"Application Logs\", mimeType: \"text/plain\", arguments: [ { name: \"name\", description: \"Name of the log\", required: true, complete: async (value) => { if (value === \"Example\") { return { values: [\"Example Log\"], }; } return { values: [], }; }, }, ], async load({ name }) { return { text: `Example log content for ${name}`, }; }, }); ``` ### Embedded Resources FastMCP provides a convenient `embedded()` method that simplifies including resources in tool responses. This feature reduces code duplication and makes it easier to reference resources from within tools. #### Basic Usage ```js server.addTool({ name: \"get_user_data\", description: \"Retrieve user information\", parameters: z.object({ userId: z.string(), }), execute: async (args) => { return { content: [ { type: \"resource\", resource: await server.embedded(`user://profile/${args.userId}`), }, ], }; }, }); ``` #### Working with Resource Templates The `embedded()` method works seamlessly with resource templates: ```js // Define a resource template server.addResourceTemplate({ uriTemplate: \"docs://project/{section}\", name: \"Project Documentation\", mimeType: \"text/markdown\", arguments: [ { name: \"section\", required: true, }, ], async load(args) { const docs = { \"getting-started\": \"# Getting Started\\n\\nWelcome to our project!\", \"api-reference\": \"# API Reference\\n\\nAuthentication is required.\", }; return { text: docs[args.section] || \"Documentation not found\", }; }, }); // Use embedded resources in a tool server.addTool({ name: \"get_documentation\", description: \"Retrieve project documentation\", parameters: z.object({ section: z.enum([\"getting-started\", \"api-reference\"]), }), execute: async (args) => { return { content: [ { type: \"resource\", resource: await server.embedded(`docs://project/${args.section}`), }, ], }; }, }); ``` #### Working with Direct Resources It also works with directly defined resources: ```js // Define a direct resource server.addResource({ uri: \"system://status\", name: \"System Status\", mimeType: \"text/plain\", async load() { return { text: \"System operational\", }; }, }); // Use in a tool server.addTool({ name: \"get_system_status\", description: \"Get current system status\", parameters: z.object({}), execute: async () => { return { content: [ { type: \"resource\", resource: await server.embedded(\"system://status\"), }, ], }; }, }); ``` ### Prompts [Prompts](https://modelcontextprotocol.io/docs/concepts/prompts) enable servers to define reusable prompt templates and workflows that clients can easily surface to users and LLMs. They provide a powerful way to standardize and share common LLM interactions. ```ts server.addPrompt({ name: \"git-commit\", description: \"Generate a Git commit message\", arguments: [ { name: \"changes\", description: \"Git diff or description of changes\", required: true, }, ], load: async (args) => { return `Generate a concise but descriptive commit message for these changes:\\n\\n${args.changes}`; }, }); ``` #### Prompt argument auto-completion Prompts can provide auto-completion for their arguments: ```js server.addPrompt({ name: \"countryPoem\", description: \"Writes a poem about a country\", load: async ({ name }) => { return `Hello, ${name}!`; }, arguments: [ { name: \"name\", description: \"Name of the country\", required: true, complete: async (value) => { if (value === \"Germ\") { return { values: [\"Germany\"], }; } return { values: [], }; }, }, ], }); ``` #### Prompt argument auto-completion using `enum` If you provide an `enum` array for an argument, the server will automatically provide completions for the argument. ```js server.addPrompt({ name: \"countryPoem\", description: \"Writes a poem about a country\", load: async ({ name }) => { return `Hello, ${name}!`; }, arguments: [ { name: \"name\", description: \"Name of the country\", required: true, enum: [\"Germany\", \"France\", \"Italy\"], }, ], }); ``` ### Authentication FastMCP supports session-based authentication, allowing you to secure your server and control access to its features. > [!NOTE] > For more granular control over which tools are available to authenticated users, see the [Tool Authorization](#tool-authorization) section. To enable authentication, provide an `authenticate` function in the server options. This function receives the incoming HTTP request and should return a promise that resolves with the authentication context. If authentication fails, the function should throw a `Response` object, which will be sent to the client. ```ts const server = new FastMCP({ name: \"My Server\", version: \"1.0.0\", authenticate: (request) => { const apiKey = request.headers[\"x-api-key\"]; if (apiKey !== \"123\") { throw new Response(null, { status: 401, statusText: \"Unauthorized\", }); } // Whatever you return here will be accessible in the `context.session` object. return { id: 1, }; }, }); ``` Now you can access the authenticated session data in your tools: ```ts server.addTool({ name: \"sayHello\", execute: async (args, { session }) => { return `Hello, ${session.id}!`; }, }); ``` #### Tool Authorization You can control which tools are available to authenticated users by adding an optional `canAccess` function to a tool's definition. This function receives the authentication context and should return `true` if the user is allowed to access the tool. If `canAccess` is not provided, the tool is accessible to all authenticated users by default. If no authentication is configured on the server, all tools are available to all clients. **Example:** ```typescript const server = new FastMCP ({ authenticate: async (request) => { const role = request.headers[\"x-role\"] as string; return { role: role === \"admin\" ? \"admin\" : \"user\" }; }, name: \"My Server\", version: \"1.0.0\", }); server.addTool({ name: \"admin-dashboard\", description: \"An admin-only tool\", // Only users with the 'admin' role can see and execute this tool canAccess: (auth) => auth?.role === \"admin\", execute: async () => { return \"Welcome to the admin dashboard!\"; }, }); server.addTool({ name: \"public-info\", description: \"A tool available to everyone\", execute: async () => { return \"This is public information.\"; }, }); ``` In this example, only clients authenticating with the `admin` role will be able to list or call the `admin-dashboard` tool. The `public-info` tool will be available to all authenticated users. #### OAuth Support FastMCP includes built-in support for OAuth discovery endpoints, supporting both **MCP Specification 2025-03-26** and **MCP Specification 2025-06-18** for OAuth integration. This makes it easy to integrate with OAuth authorization flows by providing standard discovery endpoints that comply with RFC 8414 (OAuth 2.0 Authorization Server Metadata) and RFC 9470 (OAuth 2.0 Protected Resource Metadata): ```ts import { FastMCP, DiscoveryDocumentCache } from \"fastmcp\"; import { buildGetJwks } from \"get-jwks\"; import fastJwt from \"fast-jwt\"; // Create a cache for discovery documents (reuse across requests) const discoveryCache = new DiscoveryDocumentCache({ ttl: 3600000, // Cache for 1 hour (default) }); const server = new FastMCP({ name: \"My Server\", version: \"1.0.0\", oauth: { enabled: true, authorizationServer: { issuer: \"https://auth.example.com\", authorizationEndpoint: \"https://auth.example.com/oauth/authorize\", tokenEndpoint: \"https://auth.example.com/oauth/token\", jwksUri: \"https://auth.example.com/.well-known/jwks.json\", responseTypesSupported: [\"code\"], }, protectedResource: { resource: \"mcp://my-server\", authorizationServers: [\"https://auth.example.com\"], }, }, authenticate: async (request) => { const authHeader = request.headers.authorization; if (!authHeader?.startsWith(\"Bearer \")) { throw new Response(null, { status: 401, statusText: \"Missing or invalid authorization header\", }); } const token = authHeader.slice(7); // Remove 'Bearer ' prefix // Validate OAuth JWT access token using OpenID Connect discovery try { // Fetch and cache the discovery document const discoveryUrl = \"https://auth.example.com/.well-known/openid-configuration\"; // Alternative: Use OAuth authorization server metadata endpoint // const discoveryUrl = 'https://auth.example.com/.well-known/oauth-authorization-server'; const config = (await discoveryCache.get(discoveryUrl)) as { jwks_uri: string; issuer: string; }; const jwksUri = config.jwks_uri; const issuer = config.issuer; // Create JWKS client for token verification using discovered endpoint const getJwks = buildGetJwks({ jwksUrl: jwksUri, cache: true, rateLimit: true, }); // Create JWT verifier with JWKS and discovered issuer const verify = fastJwt.createVerifier({ key: async (token) => { const { header } = fastJwt.decode(token, { complete: true }); const jwk = await getJwks.getJwk({ kid: header.kid, alg: header.alg, }); return jwk; }, algorithms: [\"RS256\", \"ES256\"], issuer: issuer, audience: \"mcp://my-server\", }); // Verify the JWT token const payload = await verify(token); return { userId: payload.sub, scope: payload.scope, email: payload.email, // Include other claims as needed }; } catch (error) { throw new Response(null, { status: 401, statusText: \"Invalid OAuth token\", }); } }, }); ``` This configuration automatically exposes OAuth discovery endpoints: - `/.well-known/oauth-authorization-server` - Authorization server metadata (RFC 8414) - `/.well-known/oauth-protected-resource` - Protected resource metadata (RFC 9470) For JWT token validation, you can use libraries like [`get-jwks`](https://github.com/nearform/get-jwks) and [`@fastify/jwt`](https://github.com/fastify/fastify-jwt) for OAuth JWT tokens. #### Passing Headers Through Context If you are exposing your MCP server via HTTP, you may wish to allow clients to supply sensitive keys via headers, which can then be passed along to APIs that your tools interact with, allowing each client to supply their own API keys. This can be done by capturing the HTTP headers in the `authenticate` section and storing them in the session to be referenced by the tools later. ```ts import { FastMCP } from \"fastmcp\"; import { IncomingHttpHeaders } from \"http\"; // Define the session data type interface SessionData { headers: IncomingHttpHeaders; [key: string]: unknown; // Add index signature to satisfy Record } // Create a server instance const server = new FastMCP({ name: \"My Server\", version: \"1.0.0\", authenticate: async (request: any): Promise => { // Authentication logic return { headers: request.headers, }; }, }); // Tool to display HTTP headers server.addTool({ name: \"headerTool\", description: \"Reads HTTP headers from the request\", execute: async (args: any, context: any) => { const session = context.session as SessionData; const headers = session?.headers ?? {}; const getHeaderString = (header: string | string[] | undefined) => Array.isArray(header) ? header.join(\", \") : (header ?? \"N/A\"); const userAgent = getHeaderString(headers[\"user-agent\"]); const authorization = getHeaderString(headers[\"authorization\"]); return `User-Agent: ${userAgent}\\nAuthorization: ${authorization}\\nAll Headers: ${JSON.stringify(headers, null, 2)}`; }, }); // Start the server server.start({ transportType: \"httpStream\", httpStream: { port: 8080, }, }); ``` A client that would connect to this may look something like this: ```ts import { StreamableHTTPClientTransport } from \"@modelcontextprotocol/sdk/client/streamableHttp.js\"; import { Client } from \"@modelcontextprotocol/sdk/client/index.js\"; const transport = new StreamableHTTPClientTransport( new URL(`http://localhost:8080/mcp`), { requestInit: { headers: { Authorization: \"Test 123\", }, }, }, ); const client = new Client({ name: \"example-client\", version: \"1.0.0\", }); (async () => { await client.connect(transport); // Call a tool const result = await client.callTool({ name: \"headerTool\", arguments: { arg1: \"value\", }, }); console.log(\"Tool result:\", result); })().catch(console.error); ``` What would show up in the console after the client runs is something like this: ``` Tool result: { content: [ { type: 'text', text: 'User-Agent: node\\n' + 'Authorization: Test 123\\n' + 'All Headers: {\\n' + ' \"host\": \"localhost:8080\",\\n' + ' \"connection\": \"keep-alive\",\\n' + ' \"authorization\": \"Test 123\",\\n' + ' \"content-type\": \"application/json\",\\n' + ' \"accept\": \"application/json, text/event-stream\",\\n' + ' \"accept-language\": \"*\",\\n' + ' \"sec-fetch-mode\": \"cors\",\\n' + ' \"user-agent\": \"node\",\\n' + ' \"accept-encoding\": \"gzip, deflate\",\\n' + ' \"content-length\": \"163\"\\n' + '}' } ] } ``` #### Session ID and Request ID Tracking FastMCP automatically exposes session and request IDs to tool handlers through the context parameter. This enables per-session state management and request tracking. **Session ID** (`context.sessionId`): - Available only for HTTP-based transports (HTTP Stream, SSE) - Extracted from the `Mcp-Session-Id` header - Remains constant across multiple requests from the same client - Useful for maintaining per-session state, counters, or user-specific data **Request ID** (`context.requestId`): - Available for all transports when provided by the client - Unique for each individual request - Useful for request tracing and debugging ```ts import { FastMCP } from \"fastmcp\"; import { z } from \"zod\"; const server = new FastMCP({ name: \"Session Counter Server\", version: \"1.0.0\", }); // Per-session counter storage const sessionCounters = new Map (); server.addTool({ name: \"increment_counter\", description: \"Increment a per-session counter\", parameters: z.object({}), execute: async (args, context) => { if (!context.sessionId) { return \"Session ID not available (requires HTTP transport)\"; } const counter = sessionCounters.get(context.sessionId) || 0; const newCounter = counter + 1; sessionCounters.set(context.sessionId, newCounter); return `Counter for session ${context.sessionId}: ${newCounter}`; }, }); server.addTool({ name: \"show_ids\", description: \"Display session and request IDs\", parameters: z.object({}), execute: async (args, context) => { return `Session ID: ${context.sessionId || \"N/A\"} Request ID: ${context.requestId || \"N/A\"}`; }, }); server.start({ transportType: \"httpStream\", httpStream: { port: 8080, }, }); ``` **Use Cases:** - **Per-session state management**: Maintain counters, caches, or temporary data unique to each client session - **User authentication and authorization**: Track authenticated users across requests - **Session-specific resource management**: Allocate and manage resources per session - **Multi-tenant implementations**: Isolate data and operations by session - **Request tracing**: Track individual requests for debugging and monitoring **Example:** See [`src/examples/session-id-counter.ts`](src/examples/session-id-counter.ts) for a complete example demonstrating session-based counter management. **Notes:** - Session IDs are automatically generated by the MCP transport layer - In stateless mode, session IDs are not persisted across requests - For stdio transport, `sessionId` will be `undefined` as there's no HTTP session concept ### Providing Instructions You can provide instructions to the server using the `instructions` option: ```ts const server = new FastMCP({ name: \"My Server\", version: \"1.0.0\", instructions: 'Instructions describing how to use the server and its features.\\n\\nThis can be used by clients to improve the LLM\\'s understanding of available tools, resources, etc. It can be thought of like a \"hint\" to the model. For example, this information MAY be added to the system prompt.', }); ``` ### Sessions The `session` object is an instance of `FastMCPSession` and it describes active client sessions. ```ts server.sessions; ``` We allocate a new server instance for each client connection to enable 1:1 communication between a client and the server. ### Typed server events You can listen to events emitted by the server using the `on` method: ```ts server.on(\"connect\", (event) => { console.log(\"Client connected:\", event.session); }); server.on(\"disconnect\", (event) => { console.log(\"Client disconnected:\", event.session); }); ``` ## `FastMCPSession` `FastMCPSession` represents a client session and provides methods to interact with the client. Refer to [Sessions](#sessions) for examples of how to obtain a `FastMCPSession` instance. ### `requestSampling` `requestSampling` creates a [sampling](https://modelcontextprotocol.io/docs/concepts/sampling) request and returns the response. ```ts await session.requestSampling({ messages: [ { role: \"user\", content: { type: \"text\", text: \"What files are in the current directory?\", }, }, ], systemPrompt: \"You are a helpful file system assistant.\", includeContext: \"thisServer\", maxTokens: 100, }); ``` #### Options `requestSampling` accepts an optional second parameter for request options: ```ts await session.requestSampling( { messages: [ { role: \"user\", content: { type: \"text\", text: \"What files are in the current directory?\", }, }, ], systemPrompt: \"You are a helpful file system assistant.\", includeContext: \"thisServer\", maxTokens: 100, }, { // Progress callback - called when progress notifications are received onprogress: (progress) => { console.log(`Progress: ${progress.progress}/${progress.total}`); }, // Abort signal for cancelling the request signal: abortController.signal, // Request timeout in milliseconds (default: DEFAULT_REQUEST_TIMEOUT_MSEC) timeout: 30000, // Whether progress notifications reset the timeout (default: false) resetTimeoutOnProgress: true, // Maximum total timeout regardless of progress (no default) maxTotalTimeout: 60000, }, ); ``` **Options:** - `onprogress?: (progress: Progress) => void` - Callback for progress notifications from the remote end - `signal?: AbortSignal` - Abort signal to cancel the request - `timeout?: number` - Request timeout in milliseconds - `resetTimeoutOnProgress?: boolean` - Whether progress notifications reset the timeout - `maxTotalTimeout?: number` - Maximum total timeout regardless of progress notifications ### `clientCapabilities` The `clientCapabilities` property contains the client capabilities. ```ts session.clientCapabilities; ``` ### `loggingLevel` The `loggingLevel` property describes the logging level as set by the client. ```ts session.loggingLevel; ``` ### `roots` The `roots` property contains the roots as set by the client. ```ts session.roots; ``` ### `server` The `server` property contains an instance of MCP server that is associated with the session. ```ts session.server; ``` ### Typed session events You can listen to events emitted by the session using the `on` method: ```ts session.on(\"rootsChanged\", (event) => { console.log(\"Roots changed:\", event.roots); }); session.on(\"error\", (event) => { console.error(\"Error:\", event.error); }); ``` ## Running Your Server ### Test with `mcp-cli` The fastest way to test and debug your server is with `fastmcp dev`: ```bash npx fastmcp dev server.js npx fastmcp dev server.ts ``` This will run your server with [`mcp-cli`](https://github.com/wong2/mcp-cli) for testing and debugging your MCP server in the terminal. ### Inspect with `MCP Inspector` Another way is to use the official [`MCP Inspector`](https://modelcontextprotocol.io/docs/tools/inspector) to inspect your server with a Web UI: ```bash npx fastmcp inspect server.ts ``` ## FAQ ### How to use with Claude Desktop? Follow the guide https://modelcontextprotocol.io/quickstart/user and add the following configuration: ```json { \"mcpServers\": { \"my-mcp-server\": { \"command\": \"npx\", \"args\": [\"tsx\", \"/PATH/TO/YOUR_PROJECT/src/index.ts\"], \"env\": { \"YOUR_ENV_VAR\": \"value\" } } } } ``` ### How to run FastMCP behind a proxy? Refer to this [issue](https://github.com/punkpeye/fastmcp/issues/25#issuecomment-3004568732) for an example of using FastMCP with `express` and `http-proxy-middleware`. ## Showcase > [!NOTE] > > If you've developed a server using FastMCP, please [submit a PR](https://github.com/punkpeye/fastmcp) to showcase it here! > [!NOTE] > > If you are looking for a boilerplate repository to build your own MCP server, check out [fastmcp-boilerplate](https://github.com/punkpeye/fastmcp-boilerplate). - [apinetwork/piapi-mcp-server](https://github.com/apinetwork/piapi-mcp-server) - generate media using Midjourney/Flux/Kling/LumaLabs/Udio/Chrip/Trellis - [domdomegg/computer-use-mcp](https://github.com/domdomegg/computer-use-mcp) - controls your computer - [LiterallyBlah/Dradis-MCP](https://github.com/LiterallyBlah/Dradis-MCP) â€“ manages projects and vulnerabilities in Dradis - [Meeting-Baas/meeting-mcp](https://github.com/Meeting-Baas/meeting-mcp) - create meeting bots, search transcripts, and manage recording data - [drumnation/unsplash-smart-mcp-server](https://github.com/drumnation/unsplash-smart-mcp-server) â€“ enables AI agents to seamlessly search, recommend, and deliver professional stock photos from Unsplash - [ssmanji89/halopsa-workflows-mcp](https://github.com/ssmanji89/halopsa-workflows-mcp) - HaloPSA Workflows integration with AI assistants - [aiamblichus/mcp-chat-adapter](https://github.com/aiamblichus/mcp-chat-adapter) â€“ provides a clean interface for LLMs to use chat completion - [eyaltoledano/claude-task-master](https://github.com/eyaltoledano/claude-task-master) â€“ advanced AI project/task manager powered by FastMCP - [cswkim/discogs-mcp-server](https://github.com/cswkim/discogs-mcp-server) - connects to the Discogs API for interacting with your music collection - [Panzer-Jack/feuse-mcp](https://github.com/Panzer-Jack/feuse-mcp) - Frontend Useful MCP Tools - Essential utilities for web developers to automate API integration and code generation - [sunra-ai/sunra-clients](https://github.com/sunra-ai/sunra-clients/tree/main/mcp-server) - Sunra.ai is a generative media platform built for developers, providing high-performance AI model inference capabilities. - [foxtrottwist/shortcuts-mcp](https://github.com/foxtrottwist/shortcuts-mcp) - connects Claude to macOS Shortcuts for system automation, app integration, and interactive workflows ## Acknowledgements - FastMCP is inspired by the [Python implementation](https://github.com/jlowin/fastmcp) by [Jonathan Lowin](https://github.com/jlowin). - Parts of codebase were adopted from [LiteMCP](https://github.com/wong2/litemcp). - Parts of codebase were adopted from [Model Context protocolã§SSEã‚’ã‚„ã£ã¦ã¿ã‚‹](https://dev.classmethod.jp/articles/mcp-sse/).","headings":[]}
{"id":"fastmcp-jlowin-readme-chunk-0","pageId":"fastmcp-jlowin-readme","url":"https://github.com/jlowin/fastmcp/blob/main/README.md","title":"fastmcp - README","text":"# FastMCP v2 ðŸš€ The fast, Pythonic way to build MCP servers and clients. *Made with â˜•ï¸ by [Prefect](https://www.prefect.io/)* [![Docs](https://img.shields.io/badge/docs-gofastmcp.com-blue)](https://gofastmcp.com) [![Discord](https://img.shields.io/badge/community-discord-5865F2?logo=discord&logoColor=white)](https://discord.gg/uu8dJCgttd) [![PyPI - Version](https://img.shields.io/pypi/v/fastmcp.svg)](https://pypi.org/project/fastmcp) [![Tests](https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml/badge.svg)](https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml) [![License](https://img.shields.io/github/license/jlowin/fastmcp.svg)](https://github.com/jlowin/fastmcp/blob/main/LICENSE) > [!Note] > > #### FastMCP 2.0: The Standard Framework > > FastMCP pioneered Python MCP development, and FastMCP 1.0 was incorporated into the [official MCP SDK](https://github.com/modelcontextprotocol/python-sdk) in 2024. > > **This is FastMCP 2.0** â€” the actively maintained, production-ready framework that extends far beyond basic protocol implementation. While the SDK provides core functionality, FastMCP 2.0 delivers everything needed for production: advanced MCP patterns (server composition, proxying, OpenAPI/FastAPI generation, tool transformation), enterprise auth (Google, GitHub, WorkOS, Azure, Auth0, and more), deployment tools, testing utilities, and comprehensive client libraries. > > **For production MCP applications, install FastMCP:** `pip install fastmcp` --- **FastMCP is the standard framework for building MCP applications**, providing the fastest path from idea to production. The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) is a standardized way to provide context and tools to LLMs. FastMCP makes building production-ready MCP servers simple, with enterprise auth, deployment tools, and a complete ecosystem built in. ```python # server.py from fastmcp import FastMCP mcp = FastMCP(\"Demo ðŸš€\") @mcp.tool def add(a: int, b: int) -> int: \"\"\"Add two numbers\"\"\" return a + b if __name__ == \"__main__\": mcp.run() ``` Run the server locally: ```bash fastmcp run server.py ``` ### ðŸ“š Documentation FastMCP's complete documentation is available at **[gofastmcp.com](https://gofastmcp.com)**, including detailed guides, API references, and advanced patterns. This readme provides only a high-level overview. Documentation is also available in [llms.txt format](https://llmstxt.org/), which is a simple markdown standard that LLMs can consume easily. There are two ways to access the LLM-friendly documentation: - [`llms.txt`](https://gofastmcp.com/llms.txt) is essentially a sitemap, listing all the pages in the documentation. - [`llms-full.txt`](https://gofastmcp.com/llms-full.txt) contains the entire documentation. Note this may exceed the context window of your LLM. **Community:** Join our [Discord server](https://discord.gg/uu8dJCgttd) to connect with other FastMCP developers and share what you're building. --- ## Table of Contents - [FastMCP v2 ðŸš€](#fastmcp-v2-) - [ðŸ“š Documentation](#-documentation) - [What is MCP?](#what-is-mcp) - [Why FastMCP?](#why-fastmcp) - [Installation](#installation) - [Core Concepts](#core-concepts) - [The `FastMCP` Server](#the-fastmcp-server) - [Tools](#tools) - [Resources \\& Templates](#resources--templates) - [Prompts](#prompts) - [Context](#context) - [MCP Clients](#mcp-clients) - [Authentication](#authentication) - [Enterprise Authentication, Zero Configuration](#enterprise-authentication-zero-configuration) - [Deployment](#deployment) - [From Development to Production](#from-development-to-production) - [Advanced Features](#advanced-features) - [Proxy Servers](#proxy-servers) - [Composing MCP Servers](#composing-mcp-servers) - [OpenAPI \\& FastAPI Generation](#openapi--fastapi-generation) - [Running Your Server](#running-your-server) - [Contributing](#contributing) - [Prerequisites](#prerequisites) - [Setup](#setup) - [Unit Tests](#unit-tests) - [Static Checks](#static-checks) - [Pull Requests](#pull-requests) --- ## What is MCP? The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. It is often described as \"the USB-C port for AI\", providing a uniform way to connect LLMs to resources they can use. It may be easier to think of it as an API, but specifically designed for LLM interactions. MCP servers can: - Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM's context) - Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect) - Define interaction patterns through **Prompts** (reusable templates for LLM interactions) - And more! FastMCP provides a high-level, Pythonic interface for building, managing, and interacting with these servers. ## Why FastMCP? FastMCP handles all the complex protocol details so you can focus on building. In most cases, decorating a Python function is all you need â€” FastMCP handles the rest. ðŸš€ **Fast:** High-level interface means less code and faster development ðŸ€ **Simple:** Build MCP servers with minimal boilerplate ðŸ **Pythonic:** Feels natural to Python developers ðŸ” **Complete:** Everything for production â€” enterprise auth (Google, GitHub, Azure, Auth0, WorkOS), deployment tools, testing frameworks, client libraries, and more FastMCP provides the shortest path from idea to production. Deploy locally, to the cloud with [FastMCP Cloud](https://fastmcp.cloud), or to your own infrastructure. ## Installation We recommend installing FastMCP with [uv](https://docs.astral.sh/uv/): ```bash uv pip install fastmcp ``` For full installation instructions, including verification, upgrading from the official MCPSDK, and developer setup, see the [**Installation Guide**](https://gofastmcp.com/getting-started/installation). **Dependency Licensing:** FastMCP depends on Cyclopts for CLI functionality. Cyclopts v4 includes docutils as a transitive dependency, which has complex licensing that may trigger compliance reviews in some organizations. If this is a concern, you can install Cyclopts v5 alpha (`pip install \"cyclopts>=5.0.0a1\"`) which removes this dependency, or wait for the stable v5 release. See [this issue](https://github.com/BrianPugh/cyclopts/issues/672) for details. ## Core Concepts These are the building blocks for creating MCP servers and clients with FastMCP. ### The `FastMCP` Server The central object representing your MCP application. It holds your tools, resources, and prompts, manages connections, and can be configured with settings like authentication. ```python from fastmcp import FastMCP # Create a server instance mcp = FastMCP(name=\"MyAssistantServer\") ``` Learn more in the [**FastMCP Server Documentation**](https://gofastmcp.com/servers/fastmcp). ### Tools Tools allow LLMs to perform actions by executing your Python functions (sync or async). Ideal for computations, API calls, or side effects (like `POST`/`PUT`). FastMCP handles schema generation from type hints and docstrings. Tools can return various types, including text, JSON-serializable objects, and even images or audio aided by the FastMCP media helper classes. ```python @mcp.tool def multiply(a: float, b: float) -> float: \"\"\"Multiplies two numbers.\"\"\" return a * b ``` Learn more in the [**Tools Documentation**](https://gofastmcp.com/servers/tools). ### Resources & Templates Resources expose read-only data sources (like `GET` requests). Use `@mcp.resource(\"your://uri\")`. Use `{placeholders}` in the URI to create dynamic templates that accept parameters, allowing clients to request specific data subsets. ```python # Static resource @mcp.resource(\"config://version\") def get_version(): return \"2.0.1\" # Dynamic resource template @mcp.resource(\"users://{user_id}/profile\") def get_profile(user_id: int): # Fetch profile for user_id... return {\"name\": f\"User {user_id}\", \"status\": \"active\"} ``` Learn more in the [**Resources & Templates Documentation**](https://gofastmcp.com/servers/resources). ### Prompts Prompts define reusable message templates to guide LLM interactions. Decorate functions with `@mcp.prompt`. Return strings or `Message` objects. ```python @mcp.prompt def summarize_request(text: str) -> str: \"\"\"Generate a prompt asking for a summary.\"\"\" return f\"Please summarize the following text:\\n\\n{text}\" ``` Learn more in the [**Prompts Documentation**](https://gofastmcp.com/servers/prompts). ### Context Access MCP session capabilities within your tools, resources, or prompts by adding a `ctx: Context` parameter. Context provides methods for: - **Logging:** Log messages to MCP clients with `ctx.info()`, `ctx.error()`, etc. - **LLM Sampling:** Use `ctx.sample()` to request completions from the client's LLM. - **Resource Access:** Use `ctx.read_resource()` to access resources on the server - **Progress Reporting:** Use `ctx.report_progress()` to report progress to the client. - and more... To access the context, add a parameter annotated as `Context` to any mcp-decorated function. FastMCP will automatically inject the correct context object when the function is called. ```python from fastmcp import FastMCP, Context mcp = FastMCP(\"My MCP Server\") @mcp.tool async def process_data(uri: str, ctx: Context): # Log a message to the client await ctx.info(f\"Processing {uri}...\") # Read a resource from the server data = await ctx.read_resource(uri) # Ask client LLM to summarize the data summary = await ctx.sample(f\"Summarize: {data.content[:500]}\") # Return the summary return summary.text ``` Learn more in the [**Context Documentation**](https://gofastmcp.com/servers/context). ### MCP Clients Interact with *any* MCP server programmatically using the `fastmcp.Client`. It supports various transports (Stdio, SSE, In-Memory) and often auto-detects the correct one. The client can also handle advanced patterns like server-initiated **LLM sampling requests** if you provide an appropriate handler. Critically, the client allows for efficient **in-memory testing** of your servers by connecting directly to a `FastMCP` server instance via the `FastMCPTransport`, eliminating the need for process management or network calls during tests. ```python from fastmcp import Client async def main(): # Connect via stdio to a local script async with Client(\"my_server.py\") as client: tools = await client.list_tools() print(f\"Available tools: {tools}\") result = await client.call_tool(\"add\", {\"a\": 5, \"b\": 3}) print(f\"Result: {result.content[0].text}\") # Connect via SSE async with Client(\"http://localhost:8000/sse\") as client: # ... use the client pass ``` To use clients to test servers, use the following pattern: ```python from fastmcp import FastMCP, Client mcp = FastMCP(\"My MCP Server\") async def main(): # Connect via in-memory transport async with Client(mcp) as client: # ... use the client ``` FastMCP also supports connecting to multiple servers through a single unified client using the standard MCP configuration format: ```python from fastmcp import Client # Standard MCP configuration with multiple servers config = { \"mcpServers\": { \"weather\": {\"url\": \"https://weather-api.example.com/mcp\"}, \"assistant\": {\"command\": \"python\", \"args\": [\"./assistant_server.py\"]} } } # Create a client that connects to all servers client = Client(config) async def main(): async with client: # Access tools and resources with server prefixes forecast = await client.call_tool(\"weather_get_forecast\", {\"city\": \"London\"}) answer = await client.call_tool(\"assistant_answer_question\", {\"query\": \"What is MCP?\"}) ``` Learn more in the [**Client Documentation**](https://gofastmcp.com/clients/client) and [**Transports Documentation**](https://gofastmcp.com/clients/transports). ## Authentication ### Enterprise Authentication, Zero Configuration FastMCP provides comprehensive authentication support that sets it apart from basic MCP implementations. Secure your servers and authenticate your clients with the same enterprise-grade providers used by major corporations. **Built-in OAuth Providers:** - **Google** - **GitHub** - **Microsoft Azure** - **Auth0** - **WorkOS** - **Descope** - **JWT/Custom** - **API Keys** Protecting a server takes just two lines: ```python from fastmcp.server.auth.providers.google import GoogleProvider auth = GoogleProvider(client_id=\"...\", client_secret=\"...\", base_url=\"https://myserver.com\") mcp = FastMCP(\"Protected Server\", auth=auth) ``` Connecting to protected servers is even simpler: ```python async with Client(\"https://protected-server.com/mcp\", auth=\"oauth\") as client: # Automatic browser-based OAuth flow result = await client.call_tool(\"protected_tool\") ``` **Why FastMCP Auth Matters:** - **Production-Ready:** Persistent storage, token refresh, comprehensive error handling - **Zero-Config OAuth:** Just pass `auth=\"oauth\"` for automatic setup - **Enterprise Integration:** WorkOS SSO, Azure Active Directory, Auth0 tenants - **Developer Experience:** Automatic browser launch, local callback server, environment variable support - **Advanced Architecture:** Full OIDC support, Dynamic Client Registration (DCR), and unique OAuth proxy pattern that enables DCR with any provider *Authentication this comprehensive is unique to FastMCP 2.0.* Learn more in the **Authentication Documentation** for [servers](https://gofastmcp.com/servers/auth) and [clients](https://gofastmcp.com/clients/auth). ## Deployment ### From Development to Production FastMCP supports every deployment scenario from local development to global scale: **Development:** Run locally with a single command ```bash fastmcp run server.py ``` **Production:** Deploy to [**FastMCP Cloud**](https://fastmcp.cloud) â€” Remote MCP that just works - Instant HTTPS endpoints - Built-in authentication - Zero configuration - Free for personal servers **Self-Hosted:** Use HTTP or SSE transports for your own infrastructure ```python mcp.run(transport=\"http\", host=\"0.0.0.0\", port=8000) ``` Learn more in the [**Deployment Documentation**](https://gofastmcp.com/deployment). ## Advanced Features FastMCP introduces powerful ways to structure and compose your MCP applications. ### Proxy Servers Create a FastMCP server that acts as an intermediary for another local or remote MCP server using `FastMCP.as_proxy()`. This is especially useful for bridging transports (e.g., remote SSE to local Stdio) or adding a layer of logic to a server you don't control. Learn more in the [**Proxying Documentation**](https://gofastmcp.com/patterns/proxy). ### Composing MCP Servers Build modular applications by mounting multiple `FastMCP` instances onto a parent server using `mcp.mount()` (live link) or `mcp.import_server()` (static copy). Learn more in the [**Composition Documentation**](https://gofastmcp.com/patterns/composition). ### OpenAPI & FastAPI Generation Automatically generate FastMCP servers from existing OpenAPI specifications (`FastMCP.from_openapi()`) or FastAPI applications (`FastMCP.from_fastapi()`), instantly bringing your web APIs to the MCP ecosystem. Learn more: [**OpenAPI Integration**](https://gofastmcp.com/integrations/openapi) | [**FastAPI Integration**](https://gofastmcp.com/integrations/fastapi). ## Running Your Server The main way to run a FastMCP server is by calling the `run()` method on your server instance: ```python # server.py from fastmcp import FastMCP mcp = FastMCP(\"Demo ðŸš€\") @mcp.tool def hello(name: str) -> str: return f\"Hello, {name}!\" if __name__ == \"__main__\": mcp.run() # Default: uses STDIO transport ``` FastMCP supports three transport protocols: **STDIO (Default)**: Best for local tools and command-line scripts. ```python mcp.run(transport=\"stdio\") # Default, so transport argument is optional ``` **Streamable HTTP**: Recommended for web deployments. ```python mcp.run(transport=\"http\", host=\"127.0.0.1\", port=8000, path=\"/mcp\") ``` **SSE**: For compatibility with existing SSE clients. ```python mcp.run(transport=\"sse\", host=\"127.0.0.1\", port=8000) ``` See the [**Running Server Documentation**](https://gofastmcp.com/deployment/running-server) for more details. ## Contributing Contributions are the core of open source! We welcome improvements and features. ### Prerequisites - Python 3.10+ - [uv](https://docs.astral.sh/uv/) (Recommended for environment management) ### Setup 1. Clone the repository: ```bash git clone https://github.com/jlowin/fastmcp.git cd fastmcp ``` 2. Create and sync the environment: ```bash uv sync ``` This installs all dependencies, including dev tools. 3. Activate the virtual environment (e.g., `source .venv/bin/activate` or via your IDE). ### Unit Tests FastMCP has a comprehensive unit test suite. All PRs must introduce or update tests as appropriate and pass the full suite. Run tests using pytest: ```bash pytest ``` or if you want an overview of the code coverage ```bash uv run pytest --cov=src --cov=examples --cov-report=html ``` ### Static Checks FastMCP uses `prek` for code formatting, linting, and type-checking. All PRs must pass these checks (they run automatically in CI). Install the hooks locally: ```bash uv run prek install ``` The hooks will now run automatically on `git commit`. You can also run them manually at any time: ```bash prek run --all-files # or via uv uv run prek run --all-files ``` ### Pull Requests 1. Fork the repository on GitHub. 2. Create a feature branch from `main`. 3. Make your changes, including tests and documentation updates. 4. Ensure tests and prek hooks pass. 5. Commit your changes and push to your fork. 6. Open a pull request against the `main` branch of `jlowin/fastmcp`. Please open an issue or discussion for questions or suggestions before starting significant work!","headings":[]}
{"id":"mcp-specification-readme-chunk-0","pageId":"mcp-specification-readme","url":"https://github.com/modelcontextprotocol/specification/blob/main/README.md","title":"specification - README","text":"# Model Context Protocol (MCP) _Just heard of MCP and not sure where to start? See [the documentation website instead](https://modelcontextprotocol.io)._ This repo contains the: - MCP specification - MCP protocol schema - Official MCP documentation The schema is [defined in TypeScript](schema/2025-06-18/schema.ts) first, but [made available as JSON Schema](schema/2025-06-18/schema.json) as well, for wider compatibility. The official MCP documentation is built using Mintlify and available at [modelcontextprotocol.io](https://modelcontextprotocol.io). ## Authors The Model Context Protocol was created by David Soria Parra ([@dsp](https://github.com/dsp)) and Justin Spahr-Summers ([@jspahrsummers](https://github.com/jspahrsummers)). ## Contributing See [CONTRIBUTING.md](./CONTRIBUTING.md). ## License This project is licensed under the [MIT License](LICENSE).","headings":[]}
{"id":"ollama-readme-chunk-0","pageId":"ollama-readme","url":"https://github.com/ollama/ollama/blob/main/README.md","title":"ollama - README","text":"# Ollama Get up and running with large language models. ### macOS [Download](https://ollama.com/download/Ollama.dmg) ### Windows [Download](https://ollama.com/download/OllamaSetup.exe) ### Linux ```shell curl -fsSL https://ollama.com/install.sh | sh ``` [Manual install instructions](https://docs.ollama.com/linux#manual-install) ### Docker The official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub. ### Libraries - [ollama-python](https://github.com/ollama/ollama-python) - [ollama-js](https://github.com/ollama/ollama-js) ### Community - [Discord](https://discord.gg/ollama) - [Reddit](https://reddit.com/r/ollama) ## Quickstart To run and chat with [Gemma 3](https://ollama.com/library/gemma3): ```shell ollama run gemma3 ``` ## Model library Ollama supports a list of models available on [ollama.com/library](https://ollama.com/library 'ollama model library') Here are some example models that can be downloaded: | Model | Parameters | Size | Download | | ------------------ | ---------- | ----- | -------------------------------- | | Gemma 3 | 1B | 815MB | `ollama run gemma3:1b` | | Gemma 3 | 4B | 3.3GB | `ollama run gemma3` | | Gemma 3 | 12B | 8.1GB | `ollama run gemma3:12b` | | Gemma 3 | 27B | 17GB | `ollama run gemma3:27b` | | QwQ | 32B | 20GB | `ollama run qwq` | | DeepSeek-R1 | 7B | 4.7GB | `ollama run deepseek-r1` | | DeepSeek-R1 | 671B | 404GB | `ollama run deepseek-r1:671b` | | Llama 4 | 109B | 67GB | `ollama run llama4:scout` | | Llama 4 | 400B | 245GB | `ollama run llama4:maverick` | | Llama 3.3 | 70B | 43GB | `ollama run llama3.3` | | Llama 3.2 | 3B | 2.0GB | `ollama run llama3.2` | | Llama 3.2 | 1B | 1.3GB | `ollama run llama3.2:1b` | | Llama 3.2 Vision | 11B | 7.9GB | `ollama run llama3.2-vision` | | Llama 3.2 Vision | 90B | 55GB | `ollama run llama3.2-vision:90b` | | Llama 3.1 | 8B | 4.7GB | `ollama run llama3.1` | | Llama 3.1 | 405B | 231GB | `ollama run llama3.1:405b` | | Phi 4 | 14B | 9.1GB | `ollama run phi4` | | Phi 4 Mini | 3.8B | 2.5GB | `ollama run phi4-mini` | | Mistral | 7B | 4.1GB | `ollama run mistral` | | Moondream 2 | 1.4B | 829MB | `ollama run moondream` | | Neural Chat | 7B | 4.1GB | `ollama run neural-chat` | | Starling | 7B | 4.1GB | `ollama run starling-lm` | | Code Llama | 7B | 3.8GB | `ollama run codellama` | | Llama 2 Uncensored | 7B | 3.8GB | `ollama run llama2-uncensored` | | LLaVA | 7B | 4.5GB | `ollama run llava` | | Granite-3.3 | 8B | 4.9GB | `ollama run granite3.3` | > [!NOTE] > You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models. ## Customize a model ### Import from GGUF Ollama supports importing GGUF models in the Modelfile: 1. Create a file named `Modelfile`, with a `FROM` instruction with the local filepath to the model you want to import. ``` FROM ./vicuna-33b.Q4_0.gguf ``` 2. Create the model in Ollama ```shell ollama create example -f Modelfile ``` 3. Run the model ```shell ollama run example ``` ### Import from Safetensors See the [guide](https://docs.ollama.com/import) on importing models for more information. ### Customize a prompt Models from the Ollama library can be customized with a prompt. For example, to customize the `llama3.2` model: ```shell ollama pull llama3.2 ``` Create a `Modelfile`: ``` FROM llama3.2 # set the temperature to 1 [higher is more creative, lower is more coherent] PARAMETER temperature 1 # set the system message SYSTEM \"\"\" You are Mario from Super Mario Bros. Answer as Mario, the assistant, only. \"\"\" ``` Next, create and run the model: ``` ollama create mario -f ./Modelfile ollama run mario >>> hi Hello! It's your friend Mario. ``` For more information on working with a Modelfile, see the [Modelfile](https://docs.ollama.com/modelfile) documentation. ## CLI Reference ### Create a model `ollama create` is used to create a model from a Modelfile. ```shell ollama create mymodel -f ./Modelfile ``` ### Pull a model ```shell ollama pull llama3.2 ``` > This command can also be used to update a local model. Only the diff will be pulled. ### Remove a model ```shell ollama rm llama3.2 ``` ### Copy a model ```shell ollama cp llama3.2 my-model ``` ### Multiline input For multiline input, you can wrap text with `\"\"\"`: ``` >>> \"\"\"Hello, ... world! ... \"\"\" I'm a basic program that prints the famous \"Hello, world!\" message to the console. ``` ### Multimodal models ``` ollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\" ``` > **Output**: The image features a yellow smiley face, which is likely the central focus of the picture. ### Pass the prompt as an argument ```shell ollama run llama3.2 \"Summarize this file: $(cat README.md)\" ``` > **Output**: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications. ### Show model information ```shell ollama show llama3.2 ``` ### List models on your computer ```shell ollama list ``` ### List which models are currently loaded ```shell ollama ps ``` ### Stop a model which is currently running ```shell ollama stop llama3.2 ``` ### Generate embeddings from the CLI ```shell ollama run embeddinggemma \"Your text to embed\" ``` You can also pipe text for scripted workflows: ```shell echo \"Your text to embed\" | ollama run embeddinggemma ``` ### Start Ollama `ollama serve` is used when you want to start ollama without running the desktop application. ## Building See the [developer guide](https://github.com/ollama/ollama/blob/main/docs/development.md) ### Running local builds Next, start the server: ```shell ./ollama serve ``` Finally, in a separate shell, run a model: ```shell ./ollama run llama3.2 ``` ## REST API Ollama has a REST API for running and managing models. ### Generate a response ```shell curl http://localhost:11434/api/generate -d '{ \"model\": \"llama3.2\", \"prompt\":\"Why is the sky blue?\" }' ``` ### Chat with a model ```shell curl http://localhost:11434/api/chat -d '{ \"model\": \"llama3.2\", \"messages\": [ { \"role\": \"user\", \"content\": \"why is the sky blue?\" } ] }' ``` See the [API documentation](./docs/api.md) for all endpoints. ## Community Integrations ### Web & Desktop - [Open WebUI](https://github.com/open-webui/open-webui) - [SwiftChat (macOS with ReactNative)](https://github.com/aws-samples/swift-chat) - [Enchanted (macOS native)](https://github.com/AugustDev/enchanted) - [Hollama](https://github.com/fmaclen/hollama) - [Lollms WebUI (Single user)](https://github.com/ParisNeo/lollms-webui) - [Lollms (Multi users)](https://github.com/ParisNeo/lollms) - [LibreChat](https://github.com/danny-avila/LibreChat) - [Bionic GPT](https://github.com/bionic-gpt/bionic-gpt) - [HTML UI](https://github.com/rtcfirefly/ollama-ui) - [AI-UI](https://github.com/bajahaw/ai-ui) - [Saddle](https://github.com/jikkuatwork/saddle) - [TagSpaces](https://www.tagspaces.org) (A platform for file-based apps, [utilizing Ollama](https://docs.tagspaces.org/ai/) for the generation of tags and descriptions) - [Chatbot UI](https://github.com/ivanfioravanti/chatbot-ollama) - [Chatbot UI v2](https://github.com/mckaywrigley/chatbot-ui) - [Typescript UI](https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file) - [Minimalistic React UI for Ollama Models](https://github.com/richawo/minimal-llm-ui) - [Ollamac](https://github.com/kevinhermawan/Ollamac) - [big-AGI](https://github.com/enricoros/big-AGI) - [Cheshire Cat assistant framework](https://github.com/cheshire-cat-ai/core) - [Amica](https://github.com/semperai/amica) - [chatd](https://github.com/BruceMacD/chatd) - [Ollama-SwiftUI](https://github.com/kghandour/Ollama-SwiftUI) - [Dify.AI](https://github.com/langgenius/dify) - [MindMac](https://mindmac.app) - [NextJS Web Interface for Ollama](https://github.com/jakobhoeg/nextjs-ollama-llm-ui) - [Msty](https://msty.app) - [Chatbox](https://github.com/Bin-Huang/Chatbox) - [WinForm Ollama Copilot](https://github.com/tgraupmann/WinForm_Ollama_Copilot) - [NextChat](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web) with [Get Started Doc](https://docs.nextchat.dev/models/ollama) - [Alpaca WebUI](https://github.com/mmo80/alpaca-webui) - [OllamaGUI](https://github.com/enoch1118/ollamaGUI) - [OpenAOE](https://github.com/InternLM/OpenAOE) - [Odin Runes](https://github.com/leonid20000/OdinRunes) - [LLM-X](https://github.com/mrdjohnson/llm-x) (Progressive Web App) - [AnythingLLM (Docker + MacOs/Windows/Linux native app)](https://github.com/Mintplex-Labs/anything-llm) - [Ollama Basic Chat: Uses HyperDiv Reactive UI](https://github.com/rapidarchitect/ollama_basic_chat) - [Ollama-chats RPG](https://github.com/drazdra/ollama-chats) - [IntelliBar](https://intellibar.app/) (AI-powered assistant for macOS) - [Jirapt](https://github.com/AliAhmedNada/jirapt) (Jira Integration to generate issues, tasks, epics) - [ojira](https://github.com/AliAhmedNada/ojira) (Jira chrome plugin to easily generate descriptions for tasks) - [QA-Pilot](https://github.com/reid41/QA-Pilot) (Interactive chat tool that can leverage Ollama models for rapid understanding and navigation of GitHub code repositories) - [ChatOllama](https://github.com/sugarforever/chat-ollama) (Open Source Chatbot based on Ollama with Knowledge Bases) - [CRAG Ollama Chat](https://github.com/Nagi-ovo/CRAG-Ollama-Chat) (Simple Web Search with Corrective RAG) - [RAGFlow](https://github.com/infiniflow/ragflow) (Open-source Retrieval-Augmented Generation engine based on deep document understanding) - [StreamDeploy](https://github.com/StreamDeploy-DevRel/streamdeploy-llm-app-scaffold) (LLM Application Scaffold) - [chat](https://github.com/swuecho/chat) (chat web app for teams) - [Lobe Chat](https://github.com/lobehub/lobe-chat) with [Integrating Doc](https://lobehub.com/docs/self-hosting/examples/ollama) - [Ollama RAG Chatbot](https://github.com/datvodinh/rag-chatbot.git) (Local Chat with multiple PDFs using Ollama and RAG) - [BrainSoup](https://www.nurgo-software.com/products/brainsoup) (Flexible native client with RAG & multi-agent automation) - [macai](https://github.com/Renset/macai) (macOS client for Ollama, ChatGPT, and other compatible API back-ends) - [RWKV-Runner](https://github.com/josStorer/RWKV-Runner) (RWKV offline LLM deployment tool, also usable as a client for ChatGPT and Ollama) - [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) (app to evaluate and compare models) - [Olpaka](https://github.com/Otacon/olpaka) (User-friendly Flutter Web App for Ollama) - [Casibase](https://casibase.org) (An open source AI knowledge base and dialogue system combining the latest RAG, SSO, ollama support, and multiple large language models.) - [OllamaSpring](https://github.com/CrazyNeil/OllamaSpring) (Ollama Client for macOS) - [LLocal.in](https://github.com/kartikm7/llocal) (Easy to use Electron Desktop Client for Ollama) - [Shinkai Desktop](https://github.com/dcSpark/shinkai-apps) (Two click install Local AI using Ollama + Files + RAG) - [AiLama](https://github.com/zeyoyt/ailama) (A Discord User App that allows you to interact with Ollama anywhere in Discord) - [Ollama with Google Mesop](https://github.com/rapidarchitect/ollama_mesop/) (Mesop Chat Client implementation with Ollama) - [R2R](https://github.com/SciPhi-AI/R2R) (Open-source RAG engine) - [Ollama-Kis](https://github.com/elearningshow/ollama-kis) (A simple easy-to-use GUI with sample custom LLM for Drivers Education) - [OpenGPA](https://opengpa.org) (Open-source offline-first Enterprise Agentic Application) - [Painting Droid](https://github.com/mateuszmigas/painting-droid) (Painting app with AI integrations) - [Kerlig AI](https://www.kerlig.com/) (AI writing assistant for macOS) - [AI Studio](https://github.com/MindWorkAI/AI-Studio) - [Sidellama](https://github.com/gyopak/sidellama) (browser-based LLM client) - [LLMStack](https://github.com/trypromptly/LLMStack) (No-code multi-agent framework to build LLM agents and workflows) - [BoltAI for Mac](https://boltai.com) (AI Chat Client for Mac) - [Harbor](https://github.com/av/harbor) (Containerized LLM Toolkit with Ollama as default backend) - [PyGPT](https://github.com/szczyglis-dev/py-gpt) (AI desktop assistant for Linux, Windows, and Mac) - [Alpaca](https://github.com/Jeffser/Alpaca) (An Ollama client application for Linux and macOS made with GTK4 and Adwaita) - [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT/blob/master/docs/content/platform/ollama.md) (AutoGPT Ollama integration) - [Go-CREW](https://www.jonathanhecl.com/go-crew/) (Powerful Offline RAG in Golang) - [PartCAD](https://github.com/openvmp/partcad/) (CAD model generation with OpenSCAD and CadQuery) - [Ollama4j Web UI](https://github.com/ollama4j/ollama4j-web-ui) - Java-based Web UI for Ollama built with Vaadin, Spring Boot, and Ollama4j - [PyOllaMx](https://github.com/kspviswa/pyOllaMx) - macOS application capable of chatting with both Ollama and Apple MLX models. - [Cline](https://github.com/cline/cline) - Formerly known as Claude Dev is a VS Code extension for multi-file/whole-repo coding - [Cherry Studio](https://github.com/kangfenmao/cherry-studio) (Desktop client with Ollama support) - [ConfiChat](https://github.com/1runeberg/confichat) (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption) - [Archyve](https://github.com/nickthecook/archyve) (RAG-enabling document library) - [crewAI with Mesop](https://github.com/rapidarchitect/ollama-crew-mesop) (Mesop Web Interface to run crewAI with Ollama) - [Tkinter-based client](https://github.com/chyok/ollama-gui) (Python tkinter-based Client for Ollama) - [LLMChat](https://github.com/trendy-design/llmchat) (Privacy focused, 100% local, intuitive all-in-one chat interface) - [Local Multimodal AI Chat](https://github.com/Leon-Sander/Local-Multimodal-AI-Chat) (Ollama-based LLM Chat with support for multiple features, including PDF RAG, voice chat, image-based interactions, and integration with OpenAI.) - [ARGO](https://github.com/xark-argo/argo) (Locally download and run Ollama and Huggingface models with RAG and deep research on Mac/Windows/Linux) - [OrionChat](https://github.com/EliasPereirah/OrionChat) - OrionChat is a web interface for chatting with different AI providers - [G1](https://github.com/bklieger-groq/g1) (Prototype of using prompting strategies to improve the LLM's reasoning through o1-like reasoning chains.) - [Web management](https://github.com/lemonit-eric-mao/ollama-web-management) (Web management page) - [Promptery](https://github.com/promptery/promptery) (desktop client for Ollama.) - [Ollama App](https://github.com/JHubi1/ollama-app) (Modern and easy-to-use multi-platform client for Ollama) - [chat-ollama](https://github.com/annilq/chat-ollama) (a React Native client for Ollama) - [SpaceLlama](https://github.com/tcsenpai/spacellama) (Firefox and Chrome extension to quickly summarize web pages with ollama in a sidebar) - [YouLama](https://github.com/tcsenpai/youlama) (Webapp to quickly summarize any YouTube video, supporting Invidious as well) - [DualMind](https://github.com/tcsenpai/dualmind) (Experimental app allowing two models to talk to each other in the terminal or in a web interface) - [ollamarama-matrix](https://github.com/h1ddenpr0cess20/ollamarama-matrix) (Ollama chatbot for the Matrix chat protocol) - [ollama-chat-app](https://github.com/anan1213095357/ollama-chat-app) (Flutter-based chat app) - [Perfect Memory AI](https://www.perfectmemory.ai/) (Productivity AI assists personalized by what you have seen on your screen, heard, and said in the meetings) - [Hexabot](https://github.com/hexastack/hexabot) (A conversational AI builder) - [Reddit Rate](https://github.com/rapidarchitect/reddit_analyzer) (Search and Rate Reddit topics with a weighted summation) - [OpenTalkGpt](https://github.com/adarshM84/OpenTalkGpt) (Chrome Extension to manage open-source models supported by Ollama, create custom models, and chat with models from a user-friendly UI) - [VT](https://github.com/vinhnx/vt.ai) (A minimal multimodal AI chat app, with dynamic conversation routing. Supports local models via Ollama) - [Nosia](https://github.com/nosia-ai/nosia) (Easy to install and use RAG platform based on Ollama) - [Witsy](https://github.com/nbonamy/witsy) (An AI Desktop application available for Mac/Windows/Linux) - [Abbey](https://github.com/US-Artificial-Intelligence/abbey) (A configurable AI interface server with notebooks, document storage, and YouTube support) - [Minima](https://github.com/dmayboroda/minima) (RAG with on-premises or fully local workflow) - [aidful-ollama-model-delete](https://github.com/AidfulAI/aidful-ollama-model-delete) (User interface for simplified model cleanup) - [Perplexica](https://github.com/ItzCrazyKns/Perplexica) (An AI-powered search engine & an open-source alternative to Perplexity AI) - [Ollama Chat WebUI for Docker ](https://github.com/oslook/ollama-webui) (Support for local docker deployment, lightweight ollama webui) - [AI Toolkit for Visual Studio Code](https://aka.ms/ai-tooklit/ollama-docs) (Microsoft-official VS Code extension to chat, test, evaluate models with Ollama support, and use them in your AI applications.) - [MinimalNextOllamaChat](https://github.com/anilkay/MinimalNextOllamaChat) (Minimal Web UI for Chat and Model Control) - [Chipper](https://github.com/TilmanGriesel/chipper) AI interface for tinkerers (Ollama, Haystack RAG, Python) - [ChibiChat](https://github.com/CosmicEventHorizon/ChibiChat) (Kotlin-based Android app to chat with Ollama and Koboldcpp API endpoints) - [LocalLLM](https://github.com/qusaismael/localllm) (Minimal Web-App to run ollama models on it with a GUI) - [Ollamazing](https://github.com/buiducnhat/ollamazing) (Web extension to run Ollama models) - [OpenDeepResearcher-via-searxng](https://github.com/benhaotang/OpenDeepResearcher-via-searxng) (A Deep Research equivalent endpoint with Ollama support for running locally) - [AntSK](https://github.com/AIDotNet/AntSK) (Out-of-the-box & Adaptable RAG Chatbot) - [MaxKB](https://github.com/1Panel-dev/MaxKB/) (Ready-to-use & flexible RAG Chatbot) - [yla](https://github.com/danielekp/yla) (Web interface to freely interact with your customized models) - [LangBot](https://github.com/RockChinQ/LangBot) (LLM-based instant messaging bots platform, with Agents, RAG features, supports multiple platforms) - [1Panel](https://github.com/1Panel-dev/1Panel/) (Web-based Linux Server Management Tool) - [AstrBot](https://github.com/Soulter/AstrBot/) (User-friendly LLM-based multi-platform chatbot with a WebUI, supporting RAG, LLM agents, and plugins integration) - [Reins](https://github.com/ibrahimcetin/reins) (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.) - [Flufy](https://github.com/Aharon-Bensadoun/Flufy) (A beautiful chat interface for interacting with Ollama's API. Built with React, TypeScript, and Material-UI.) - [Ellama](https://github.com/zeozeozeo/ellama) (Friendly native app to chat with an Ollama instance) - [screenpipe](https://github.com/mediar-ai/screenpipe) Build agents powered by your screen history - [Ollamb](https://github.com/hengkysteen/ollamb) (Simple yet rich in features, cross-platform built with Flutter and designed for Ollama. Try the [web demo](https://hengkysteen.github.io/demo/ollamb/).) - [Writeopia](https://github.com/Writeopia/Writeopia) (Text editor with integration with Ollama) - [AppFlowy](https://github.com/AppFlowy-IO/AppFlowy) (AI collaborative workspace with Ollama, cross-platform and self-hostable) - [Lumina](https://github.com/cushydigit/lumina.git) (A lightweight, minimal React.js frontend for interacting with Ollama servers) - [Tiny Notepad](https://pypi.org/project/tiny-notepad) (A lightweight, notepad-like interface to chat with ollama available on PyPI) - [macLlama (macOS native)](https://github.com/hellotunamayo/macLlama) (A native macOS GUI application for interacting with Ollama models, featuring a chat interface.) - [GPTranslate](https://github.com/philberndt/GPTranslate) (A fast and lightweight, AI powered desktop translation application written with Rust and Tauri. Features real-time translation with OpenAI/Azure/Ollama.) - [ollama launcher](https://github.com/NGC13009/ollama-launcher) (A launcher for Ollama, aiming to provide users with convenient functions such as ollama server launching, management, or configuration.) - [ai-hub](https://github.com/Aj-Seven/ai-hub) (AI Hub supports multiple models via API keys and Chat support via Ollama API.) - [Mayan EDMS](https://gitlab.com/mayan-edms/mayan-edms) (Open source document management system to organize, tag, search, and automate your files with powerful Ollama driven workflows.) - [Serene Pub](https://github.com/doolijb/serene-pub) (Beginner friendly, open source AI Roleplaying App for Windows, Mac OS and Linux. Search, download and use models with Ollama all inside the app.) - [Andes](https://github.com/aqerd/andes) (A Visual Studio Code extension that provides a local UI interface for Ollama models) - [KDeps](https://github.com/kdeps/kdeps) (Kdeps is an offline-first AI framework for building Dockerized full-stack AI applications declaratively using Apple PKL and integrates APIs with Ollama on the backend.) - [Clueless](https://github.com/KashyapTan/clueless) (Open Source & Local Cluely: A desktop application LLM assistant to help you talk to anything on your screen using locally served Ollama models. Also undetectable to screenshare) - [ollama-co2](https://github.com/carbonatedWaterOrg/ollama-co2) (FastAPI web interface for monitoring and managing local and remote Ollama servers with real-time model monitoring and concurrent downloads) - [Hillnote](https://hillnote.com) (A Markdown-first workspace designed to supercharge your AI workflow. Create documents ready to integrate with Claude, ChatGPT, Gemini, Cursor, and more - all while keeping your work on your device.) ### Cloud - [Google Cloud](https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-ollama) - [Fly.io](https://fly.io/docs/python/do-more/add-ollama/) - [Koyeb](https://www.koyeb.com/deploy/ollama) ### Tutorial - [handy-ollama](https://github.com/datawhalechina/handy-ollama) (Chinese Tutorial for Ollama by [Datawhale ](https://github.com/datawhalechina) - China's Largest Open Source AI Learning Community) ### Terminal - [oterm](https://github.com/ggozad/oterm) - [Ellama Emacs client](https://github.com/s-kostyaev/ellama) - [Emacs client](https://github.com/zweifisch/ollama) - [neollama](https://github.com/paradoxical-dev/neollama) UI client for interacting with models from within Neovim - [gen.nvim](https://github.com/David-Kunz/gen.nvim) - [ollama.nvim](https://github.com/nomnivore/ollama.nvim) - [ollero.nvim](https://github.com/marco-souza/ollero.nvim) - [ollama-chat.nvim](https://github.com/gerazov/ollama-chat.nvim) - [ogpt.nvim](https://github.com/huynle/ogpt.nvim) - [gptel Emacs client](https://github.com/karthink/gptel) - [Oatmeal](https://github.com/dustinblackman/oatmeal) - [cmdh](https://github.com/pgibler/cmdh) - [ooo](https://github.com/npahlfer/ooo) - [shell-pilot](https://github.com/reid41/shell-pilot)(Interact with models via pure shell scripts on Linux or macOS) - [tenere](https://github.com/pythops/tenere) - [llm-ollama](https://github.com/taketwo/llm-ollama) for [Datasette's LLM CLI](https://llm.datasette.io/en/stable/). - [typechat-cli](https://github.com/anaisbetts/typechat-cli) - [ShellOracle](https://github.com/djcopley/ShellOracle) - [tlm](https://github.com/yusufcanb/tlm) - [podman-ollama](https://github.com/ericcurtin/podman-ollama) - [gollama](https://github.com/sammcj/gollama) - [ParLlama](https://github.com/paulrobello/parllama) - [Ollama eBook Summary](https://github.com/cognitivetech/ollama-ebook-summary/) - [Ollama Mixture of Experts (MOE) in 50 lines of code](https://github.com/rapidarchitect/ollama_moe) - [vim-intelligence-bridge](https://github.com/pepo-ec/vim-intelligence-bridge) Simple interaction of \"Ollama\" with the Vim editor - [x-cmd ollama](https://x-cmd.com/mod/ollama) - [bb7](https://github.com/drunkwcodes/bb7) - [SwollamaCLI](https://github.com/marcusziade/Swollama) bundled with the Swollama Swift package. [Demo](https://github.com/marcusziade/Swollama?tab=readme-ov-file#cli-usage) - [aichat](https://github.com/sigoden/aichat) All-in-one LLM CLI tool featuring Shell Assistant, Chat-REPL, RAG, AI tools & agents, with access to OpenAI, Claude, Gemini, Ollama, Groq, and more. - [PowershAI](https://github.com/rrg92/powershai) PowerShell module that brings AI to terminal on Windows, including support for Ollama - [DeepShell](https://github.com/Abyss-c0re/deepshell) Your self-hosted AI assistant. Interactive Shell, Files and Folders analysis. - [orbiton](https://github.com/xyproto/orbiton) Configuration-free text editor and IDE with support for tab completion with Ollama. - [orca-cli](https://github.com/molbal/orca-cli) Ollama Registry CLI Application - Browse, pull, and download models from Ollama Registry in your terminal. - [GGUF-to-Ollama](https://github.com/jonathanhecl/gguf-to-ollama) - Importing GGUF to Ollama made easy (multiplatform) - [AWS-Strands-With-Ollama](https://github.com/rapidarchitect/ollama_strands) - AWS Strands Agents with Ollama Examples - [ollama-multirun](https://github.com/attogram/ollama-multirun) - A bash shell script to run a single prompt against any or all of your locally installed ollama models, saving the output and performance statistics as easily navigable web pages. ([Demo](https://attogram.github.io/ai_test_zone/)) - [ollama-bash-toolshed](https://github.com/attogram/ollama-bash-toolshed) - Bash scripts to chat with tool using models. Add new tools to your shed with ease. Runs on Ollama. - [hle-eval-ollama](https://github.com/mags0ft/hle-eval-ollama) - Runs benchmarks like \"Humanity's Last Exam\" (HLE) on your favorite local Ollama models and evaluates the quality of their responses - [VT Code](https://github.com/vinhnx/vtcode) - VT Code is a Rust-based terminal coding agent with semantic code intelligence via Tree-sitter. Ollama integration for running local/cloud models with configurable endpoints. ### Apple Vision Pro - [SwiftChat](https://github.com/aws-samples/swift-chat) (Cross-platform AI chat app supporting Apple Vision Pro via \"Designed for iPad\") - [Enchanted](https://github.com/AugustDev/enchanted) ### Database - [pgai](https://github.com/timescale/pgai) - PostgreSQL as a vector database (Create and search embeddings from Ollama models using pgvector) - [Get started guide](https://github.com/timescale/pgai/blob/main/docs/vectorizer-quick-start.md) - [MindsDB](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/ollama_handler/README.md) (Connects Ollama models with nearly 200 data platforms and apps) - [chromem-go](https://github.com/philippgille/chromem-go/blob/v0.5.0/embed_ollama.go) with [example](https://github.com/philippgille/chromem-go/tree/v0.5.0/examples/rag-wikipedia-ollama) - [Kangaroo](https://github.com/dbkangaroo/kangaroo) (AI-powered SQL client and admin tool for popular databases) ### Package managers - [Pacman](https://archlinux.org/packages/extra/x86_64/ollama/) - [Gentoo](https://github.com/gentoo/guru/tree/master/app-misc/ollama) - [Homebrew](https://formulae.brew.sh/formula/ollama) - [Helm Chart](https://artifacthub.io/packages/helm/ollama-helm/ollama) - [Guix channel](https://codeberg.org/tusharhero/ollama-guix) - [Nix package](https://search.nixos.org/packages?show=ollama&from=0&size=50&sort=relevance&type=packages&query=ollama) - [Flox](https://flox.dev/blog/ollama-part-one) ### Libraries - [LangChain](https://python.langchain.com/docs/integrations/chat/ollama/) and [LangChain.js](https://js.langchain.com/docs/integrations/chat/ollama/) with [example](https://js.langchain.com/docs/tutorials/local_rag/) - [Firebase Genkit](https://firebase.google.com/docs/genkit/plugins/ollama) - [crewAI](https://github.com/crewAIInc/crewAI) - [Yacana](https://remembersoftwares.github.io/yacana/) (User-friendly multi-agent framework for brainstorming and executing predetermined flows with built-in tool integration) - [Strands Agents](https://github.com/strands-agents/sdk-python) (A model-driven approach to building AI agents in just a few lines of code) - [Spring AI](https://github.com/spring-projects/spring-ai) with [reference](https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html) and [example](https://github.com/tzolov/ollama-tools) - [LangChainGo](https://github.com/tmc/langchaingo/) with [example](https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example) - [LangChain4j](https://github.com/langchain4j/langchain4j) with [example](https://github.com/langchain4j/langchain4j-examples/tree/main/ollama-examples/src/main/java) - [LangChainRust](https://github.com/Abraxas-365/langchain-rust) with [example](https://github.com/Abraxas-365/langchain-rust/blob/main/examples/llm_ollama.rs) - [LangChain for .NET](https://github.com/tryAGI/LangChain) with [example](https://github.com/tryAGI/LangChain/blob/main/examples/LangChain.Samples.OpenAI/Program.cs) - [LLPhant](https://github.com/theodo-group/LLPhant?tab=readme-ov-file#ollama) - [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/llm/ollama/) and [LlamaIndexTS](https://ts.llamaindex.ai/modules/llms/available_llms/ollama) - [LiteLLM](https://github.com/BerriAI/litellm) - [OllamaFarm for Go](https://github.com/presbrey/ollamafarm) - [OllamaSharp for .NET](https://github.com/awaescher/OllamaSharp) - [Ollama for Ruby](https://github.com/gbaptista/ollama-ai) - [Ollama-rs for Rust](https://github.com/pepperoni21/ollama-rs) - [Ollama-hpp for C++](https://github.com/jmont-dev/ollama-hpp) - [Ollama4j for Java](https://github.com/ollama4j/ollama4j) - [ModelFusion Typescript Library](https://modelfusion.dev/integration/model-provider/ollama) - [OllamaKit for Swift](https://github.com/kevinhermawan/OllamaKit) - [Ollama for Dart](https://github.com/breitburg/dart-ollama) - [Ollama for Laravel](https://github.com/cloudstudio/ollama-laravel) - [LangChainDart](https://github.com/davidmigloz/langchain_dart) - [Semantic Kernel - Python](https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/ollama) - [Haystack](https://github.com/deepset-ai/haystack-integrations/blob/main/integrations/ollama.md) - [Elixir LangChain](https://github.com/brainlid/langchain) - [Ollama for R - rollama](https://github.com/JBGruber/rollama) - [Ollama for R - ollama-r](https://github.com/hauselin/ollama-r) - [Ollama-ex for Elixir](https://github.com/lebrunel/ollama-ex) - [Ollama Connector for SAP ABAP](https://github.com/b-tocs/abap_btocs_ollama) - [Testcontainers](https://testcontainers.com/modules/ollama/) - [Portkey](https://portkey.ai/docs/welcome/integration-guides/ollama) - [PromptingTools.jl](https://github.com/svilupp/PromptingTools.jl) with an [example](https://svilupp.github.io/PromptingTools.jl/dev/examples/working_with_ollama) - [LlamaScript](https://github.com/Project-Llama/llamascript) - [llm-axe](https://github.com/emirsahin1/llm-axe) (Python Toolkit for Building LLM Powered Apps) - [Gollm](https://docs.gollm.co/examples/ollama-example) - [Gollama for Golang](https://github.com/jonathanhecl/gollama) - [Ollamaclient for Golang](https://github.com/xyproto/ollamaclient) - [High-level function abstraction in Go](https://gitlab.com/tozd/go/fun) - [Ollama PHP](https://github.com/ArdaGnsrn/ollama-php) - [Agents-Flex for Java](https://github.com/agents-flex/agents-flex) with [example](https://github.com/agents-flex/agents-flex/tree/main/agents-flex-llm/agents-flex-llm-ollama/src/test/java/com/agentsflex/llm/ollama) - [Parakeet](https://github.com/parakeet-nest/parakeet) is a GoLang library, made to simplify the development of small generative AI applications with Ollama. - [Haverscript](https://github.com/andygill/haverscript) with [examples](https://github.com/andygill/haverscript/tree/main/examples) - [Ollama for Swift](https://github.com/mattt/ollama-swift) - [Swollama for Swift](https://github.com/marcusziade/Swollama) with [DocC](https://marcusziade.github.io/Swollama/documentation/swollama/) - [GoLamify](https://github.com/prasad89/golamify) - [Ollama for Haskell](https://github.com/tusharad/ollama-haskell) - [multi-llm-ts](https://github.com/nbonamy/multi-llm-ts) (A Typescript/JavaScript library allowing access to different LLM in a unified API) - [LlmTornado](https://github.com/lofcz/llmtornado) (C# library providing a unified interface for major FOSS & Commercial inference APIs) - [Ollama for Zig](https://github.com/dravenk/ollama-zig) - [Abso](https://github.com/lunary-ai/abso) (OpenAI-compatible TypeScript SDK for any LLM provider) - [Nichey](https://github.com/goodreasonai/nichey) is a Python package for generating custom wikis for your research topic - [Ollama for D](https://github.com/kassane/ollama-d) - [OllamaPlusPlus](https://github.com/HardCodeDev777/OllamaPlusPlus) (Very simple C++ library for Ollama) - [any-llm](https://github.com/mozilla-ai/any-llm) (A single interface to use different llm providers by [mozilla.ai](https://www.mozilla.ai/)) - [any-agent](https://github.com/mozilla-ai/any-agent) (A single interface to use and evaluate different agent frameworks by [mozilla.ai](https://www.mozilla.ai/)) - [Neuro SAN](https://github.com/cognizant-ai-lab/neuro-san-studio) (Data-driven multi-agent orchestration framework) with [example](https://github.com/cognizant-ai-lab/neuro-san-studio/blob/main/docs/user_guide.md#ollama) - [achatbot-go](https://github.com/ai-bot-pro/achatbot-go) a multimodal(text/audio/image) chatbot. - [Ollama Bash Lib](https://github.com/attogram/ollama-bash-lib) - A Bash Library for Ollama. Run LLM prompts straight from your shell, and more ### Mobile - [SwiftChat](https://github.com/aws-samples/swift-chat) (Lightning-fast Cross-platform AI chat app with native UI for Android, iOS, and iPad) - [Enchanted](https://github.com/AugustDev/enchanted) - [Maid](https://github.com/Mobile-Artificial-Intelligence/maid) - [Ollama App](https://github.com/JHubi1/ollama-app) (Modern and easy-to-use multi-platform client for Ollama) - [ConfiChat](https://github.com/1runeberg/confichat) (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption) - [Ollama Android Chat](https://github.com/sunshine0523/OllamaServer) (No need for Termux, start the Ollama service with one click on an Android device) - [Reins](https://github.com/ibrahimcetin/reins) (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.) ### Extensions & Plugins - [Raycast extension](https://github.com/MassimilianoPasquini97/raycast_ollama) - [Discollama](https://github.com/mxyng/discollama) (Discord bot inside the Ollama discord channel) - [Continue](https://github.com/continuedev/continue) - [Vibe](https://github.com/thewh1teagle/vibe) (Transcribe and analyze meetings with Ollama) - [Obsidian Ollama plugin](https://github.com/hinterdupfinger/obsidian-ollama) - [Logseq Ollama plugin](https://github.com/omagdy7/ollama-logseq) - [NotesOllama](https://github.com/andersrex/notesollama) (Apple Notes Ollama plugin) - [Dagger Chatbot](https://github.com/samalba/dagger-chatbot) - [Discord AI Bot](https://github.com/mekb-turtle/discord-ai-bot) - [Ollama Telegram Bot](https://github.com/ruecat/ollama-telegram) - [Hass Ollama Conversation](https://github.com/ej52/hass-ollama-conversation) - [Rivet plugin](https://github.com/abrenneke/rivet-plugin-ollama) - [Obsidian BMO Chatbot plugin](https://github.com/longy2k/obsidian-bmo-chatbot) - [Cliobot](https://github.com/herval/cliobot) (Telegram bot with Ollama support) - [Copilot for Obsidian plugin](https://github.com/logancyang/obsidian-copilot) - [Obsidian Local GPT plugin](https://github.com/pfrankov/obsidian-local-gpt) - [Open Interpreter](https://docs.openinterpreter.com/language-model-setup/local-models/ollama) - [Llama Coder](https://github.com/ex3ndr/llama-coder) (Copilot alternative using Ollama) - [Ollama Copilot](https://github.com/bernardo-bruning/ollama-copilot) (Proxy that allows you to use Ollama as a copilot like GitHub Copilot) - [twinny](https://github.com/rjmacarthy/twinny) (Copilot and Copilot chat alternative using Ollama) - [Wingman-AI](https://github.com/RussellCanfield/wingman-ai) (Copilot code and chat alternative using Ollama and Hugging Face) - [Page Assist](https://github.com/n4ze3m/page-assist) (Chrome Extension) - [Plasmoid Ollama Control](https://github.com/imoize/plasmoid-ollamacontrol) (KDE Plasma extension that allows you to quickly manage/control Ollama model) - [AI Telegram Bot](https://github.com/tusharhero/aitelegrambot) (Telegram bot using Ollama in backend) - [AI ST Completion](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (Sublime Text 4 AI assistant plugin with Ollama support) - [Discord-Ollama Chat Bot](https://github.com/kevinthedang/discord-ollama) (Generalized TypeScript Discord Bot w/ Tuning Documentation) - [ChatGPTBox: All in one browser extension](https://github.com/josStorer/chatGPTBox) with [Integrating Tutorial](https://github.com/josStorer/chatGPTBox/issues/616#issuecomment-1975186467) - [Discord AI chat/moderation bot](https://github.com/rapmd73/Companion) Chat/moderation bot written in python. Uses Ollama to create personalities. - [Headless Ollama](https://github.com/nischalj10/headless-ollama) (Scripts to automatically install ollama client & models on any OS for apps that depend on ollama server) - [Terraform AWS Ollama & Open WebUI](https://github.com/xuyangbocn/terraform-aws-self-host-llm) (A Terraform module to deploy on AWS a ready-to-use Ollama service, together with its front-end Open WebUI service.) - [node-red-contrib-ollama](https://github.com/jakubburkiewicz/node-red-contrib-ollama) - [Local AI Helper](https://github.com/ivostoykov/localAI) (Chrome and Firefox extensions that enable interactions with the active tab and customisable API endpoints. Includes secure storage for user prompts.) - [LSP-AI](https://github.com/SilasMarvin/lsp-ai) (Open-source language server for AI-powered functionality) - [QodeAssist](https://github.com/Palm1r/QodeAssist) (AI-powered coding assistant plugin for Qt Creator) - [Obsidian Quiz Generator plugin](https://github.com/ECuiDev/obsidian-quiz-generator) - [AI Summary Helper plugin](https://github.com/philffm/ai-summary-helper) - [TextCraft](https://github.com/suncloudsmoon/TextCraft) (Copilot in Word alternative using Ollama) - [Alfred Ollama](https://github.com/zeitlings/alfred-ollama) (Alfred Workflow) - [TextLLaMA](https://github.com/adarshM84/TextLLaMA) A Chrome Extension that helps you write emails, correct grammar, and translate into any language - [Simple-Discord-AI](https://github.com/zyphixor/simple-discord-ai) - [LLM Telegram Bot](https://github.com/innightwolfsleep/llm_telegram_bot) (telegram bot, primary for RP. Oobabooga-like buttons, [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) API integration e.t.c) - [mcp-llm](https://github.com/sammcj/mcp-llm) (MCP Server to allow LLMs to call other LLMs) - [SimpleOllamaUnity](https://github.com/HardCodeDev777/SimpleOllamaUnity) (Unity Engine extension for communicating with Ollama in a few lines of code. Also works at runtime) - [UnityCodeLama](https://github.com/HardCodeDev777/UnityCodeLama) (Unity Editor tool to analyze scripts via Ollama) - [NativeMind](https://github.com/NativeMindBrowser/NativeMindExtension) (Private, on-device AI Assistant, no cloud dependencies) - [GMAI - Gradle Managed AI](https://gmai.premex.se/) (Gradle plugin for automated Ollama lifecycle management during build phases) - [NOMYO Router](https://github.com/nomyo-ai/nomyo-router) (A transparent Ollama proxy with model deployment aware routing which auto-manages multiple Ollama instances in a given network) ### Supported backends - [llama.cpp](https://github.com/ggml-org/llama.cpp) project founded by Georgi Gerganov. ### Observability - [Opik](https://www.comet.com/docs/opik/cookbook/ollama) is an open-source platform to debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards. Opik supports native integration to Ollama. - [Lunary](https://lunary.ai/docs/integrations/ollama) is the leading open-source LLM observability platform. It provides a variety of enterprise-grade features such as real-time analytics, prompt templates management, PII masking, and comprehensive agent tracing. - [OpenLIT](https://github.com/openlit/openlit) is an OpenTelemetry-native tool for monitoring Ollama Applications & GPUs using traces and metrics. - [HoneyHive](https://docs.honeyhive.ai/integrations/ollama) is an AI observability and evaluation platform for AI agents. Use HoneyHive to evaluate agent performance, interrogate failures, and monitor quality in production. - [Langfuse](https://langfuse.com/docs/integrations/ollama) is an open source LLM observability platform that enables teams to collaboratively monitor, evaluate and debug AI applications. - [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing) is an open source LLM observability tool with a convenient API to log and visualize traces, making it easy to debug and evaluate GenAI applications. ### Security - [Ollama Fortress](https://github.com/ParisNeo/ollama_proxy_server)","headings":[]}